---
title: "Assignment 3"
author: "Group One"
date: "April 17, 2018"
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE , message=FALSE, warning = FALSE}
library(tidyverse)
library(visdat)
library(naniar)
library(broom)

source("https://bioconductor.org/biocLite.R")
biocLite("impute")
library(impute)

houses <- read_csv("https://dmac.netlify.com/assignments/data/Melbourne_housing_FULL.csv")  %>%
  select(Price,Rooms,Type,Distance,Bedroom2,Bathroom)
```

#Missing Value Summary
```{r}
vis_miss(houses, sort_miss=TRUE) + theme(aspect.ratio=1)
miss_summary(houses)

ggplot(houses,
       aes(x = Bathroom,
           y = Price)) +
  scale_colour_brewer(palette="Dark2") +
  geom_miss_point()+ theme(aspect.ratio=1)

ggplot(houses,
       aes(x = Bedroom2,
           y = Price)) +
  scale_colour_brewer(palette="Dark2") +
  geom_miss_point() +theme(aspect.ratio=1)
```

#Strategy
##1. Simple Strategy: Drop Observations
From the missing values summary, 40.3% of observations contain at least one missing data value. As this is a considerably high number, if we drop all of these observations, we might be missing out on valuable interpretation of the data. Rather than dropping them, we can use computation for creating additional data to fill in the missing values.

However, as price is taken as the response variable, it would not be reasonable to impute these values. From the visual summary it's seen that 21.8% of price values are missing, much lower than the 40.3% for total observations. This means we are likely not losing too much benefit from dropping these observations.

##2. Computation
Firstly, observations missing their Price value are removed.
```{r}
houses<- houses %>% 
  filter(!is.na(Price))
```

After this filtering we will still be left with over 27,000 observations, still a large sample size. Next we focus our attention on the missing bedroom2 and bathroom values.Recreating the missing data visualisations from above, the impact of missing price values has now been removed.

```{r}
ggplot(houses,
       aes(x = Bedroom2,
           y = Price)) +
  scale_colour_brewer(palette="Dark2") +
  geom_miss_point() +theme(aspect.ratio=1)

ggplot(houses,
       aes(x = Bathroom,
           y = Price)) +
  scale_colour_brewer(palette="Dark2") +
  geom_miss_point() +theme(aspect.ratio=1)

vis_miss(houses, sort_miss=TRUE) + theme(aspect.ratio=1)
miss_summary(houses)
```

From the plots we can see we have a large number of bedroom2 and bathroom values missing. While there does not appear to be strong relationships between either variable and price, we can still attempt to impute the missing values. 

It is also worth noting that 50% of variables are missing values (from the summary table) yet from the visual summary, it appears only Bathroom and Bedrooms2 are missing values. This implies that the 3rd variable in the summary, Distance, must also be missing a very small number of values. Due to the small proportion (not even 1%), these value(s) can be simply removed.

```{r}
houses <- houses %>%
  filter(!is.na(Distance))
# This removes the 1 observation missing Distance
```


##2. Imputation
Having dealt with missing Price and Distance values, the missing Bathroom and Bedroom2 values must be imputed. 2 methods will be used to complete this: mean value and nearest neighbour imputation.

# Mean Value
```{r}
tao_mean <- bind_shadow(houses)%>%
  mutate(Bedroom2 = ifelse(is.na(Bedroom2), 
                             mean(Bedroom2, na.rm=TRUE),
                             Bedroom2),
         Bathroom = ifelse(is.na(Bathroom), 
                             mean(Bathroom, na.rm=TRUE),
                             Bathroom))
ggplot(tao_mean,
       aes(x = Bedroom2,
           y = Price, 
           colour=Bedroom2_NA)) +
  geom_point(alpha=0.7)+ 
  scale_colour_brewer(palette="Dark2") +
  theme(aspect.ratio=1)

ggplot(tao_mean,
       aes(x = Bathroom,
           y = Price, 
           colour=Bathroom_NA)) +
  geom_point(alpha=0.7)+ 
  scale_colour_brewer(palette="Dark2") +
  theme(aspect.ratio=1)
```

# Using Nearest Neighbours Imputation
``` {r}
tao_impute <- bind_shadow(houses) %>%
  arrange(Price, Bedroom2, Bathroom) %>%
  select(Price, Bedroom2, Bathroom) 

#start the KNN
#need to convert to matrix
#we're using 5-NN

tao_impute <- impute.knn(as.matrix(tao_impute), 5)
tao_nn <- bind_shadow(houses) %>%
  arrange(Price, Bedroom2, Bathroom) %>%
  mutate(Bedroom2 = tao_impute$data[,2],
         Bathroom = tao_impute$data[,3])

#Looking at Final Imputation Plot

ggplot(tao_nn,
       aes(x = Bedroom2,
           y = Price,
           color = Bedroom2_NA)) +
  geom_point(alpha=0.7)+ 
  scale_colour_brewer(palette="Dark2") +
  theme(aspect.ratio=1)

ggplot(tao_nn,
       aes(x = Bathroom,
           y = Price,
           color = Bathroom_NA)) +
  geom_point(alpha=0.7)+ 
  scale_colour_brewer(palette="Dark2") +
  theme(aspect.ratio=1)

```

Comparing the 2 methods, we see very similar results. This is because the values are discrete (i.e. a house can only have an integer number of Bathrooms or Bedrooms), therefor the nearest neighbour method is effectively finding the mean for each missing value. The only differences occur with the Bathroom imputation where a small number of points (around 4) vary from the mean. Due to their similarity, there is no better choice so we will simply use the nearest neighbour imputation for the rest of the data analysis. 

In terms of the actual fit produced, the imputed values appear to fit nicely within the actual data, without any obvious outliers. If the values were not coloured, it would be almost impossible to determine which point was actual or imputed. 

The only other issue is the imputed values are not integers. However these values are simply being used to create a model. As a result of this, the model we create must take numeric inputs for both Bedroom2 and Bathroom. As these values are discrete they could potentially be converted to factors, however this would require the imputed values to also be converted to integers. For our model, numeric inputs will be used.

##Plots to visualise relationships within data
Note:
  Distance = Distance to CBD,
  Bedroom2 = Number of Bedrooms,
  Bathroom = Number of Bathrooms,
  Type: h = house (or similar, villa, cottage, etc.),
        t = townhouse,
        u = unit

```{r}
ggplot(data=tao_nn, mapping=aes(x=Type, y=Price))+
  geom_boxplot(fill = "lightblue")
```

We can see that the median price of a house is higher than that of a town house and the median price for units is the lowest. The price gap within house category is huge, which means there is a big opportunity for house to reach a very high price, relative to town house and units which have much smaller ranges. From these graphs it appears house type will have an impact in the house price hence should be included in the model

```{r}
ggplot(tao_nn, aes(x = Distance, y = Price)) + geom_point()
```

It appears that houses with less distance from the CBD tend to have more oppotunity to have high housing price. While living close is not a guarantee for high price, no houses over 25km away are priced over $3 million. As there is a relationship, Distance must also be included in the model. Due to the large right skew, this data may be potenitally require a log transformation.

```{r}
ggplot(tao_nn, aes(x=Rooms, y=Price, group=Rooms))+geom_point(alpha=0.3)
```

The distribution is skewed to the right by a few extreme values. 3 to 5 rooms are the most popular room numbers in Melbourne. There appears to be a weak positive relationship but only up to 5 rooms. There could be a potential negative quadratic relationship, with the maximum point at 4 or 5 rooms.

```{r}
ggplot(tao_nn, aes(x=Bathroom, y=Price)) + geom_point()
```

There is no clear relationship between number of bathroom and the housing price. The only point of note is houses without a bathroom sell for significantly less. This variable could be potentially converted to a binary factor, does the house have a bathroom or not.

```{r}
ggplot(tao_nn, aes(x=Bedroom2, y=Price)) + geom_point()
```

A similar relationship to Rooms is observed, with a weak positive relationship until 5 bedrooms. As this variable has a very similar shape to number of rooms, there is potential only 1 of these variables is required in the model.


# Create Model
```{r}
Data <- select(tao_nn, Price,Rooms,Type,Distance,Bedroom2,Bathroom)

# Model 1, Include all variables, no interaction terms or transformations
mod1 <- lm(Price~Distance+Bathroom+Bedroom2+Type+Rooms, data = Data)

# Model 2, Make bathroom variable binary
Data <- Data %>%
  mutate(Bathroom_1 = ifelse(Bathroom > 0, 1, 0)) #No bathroom = 0
mod2 <- lm(Price~Distance+Bathroom_1+Bedroom2+Type+Rooms, data = Data)

glance(mod1)
glance(mod2)
```

Deviance and BIC are both lower for model 1, indicating it is a better fit. The r2 value is still low however (0.41), hence further changes must be made

```{r}
Data <- select(Data, -Bathroom_1)

# Model 3, Remove Bedrooms variable, as this factor is likely included with rooms variable
mod3  <- lm(Price~Distance+Bathroom+Rooms+Type, data = Data)
glance(mod3)
```

The BIC, deviance and r2 are all very similar to model 1, even with 1 less variable.There is no point then including the Bedroom2 variable as it's simply overcomplicating the model without adding useful information. Model 3 is now the primary model  

```{r}
# Model 4, Add a quadratic term to Rooms to account for potential negative quadratic relationship
Data <- Data %>%
  mutate(Rooms2 = Rooms^2)
mod4  <- lm(Price~Distance+Bathroom+Type+Rooms+Rooms2, data = Data)
glance(mod4)
```

Again BIC, deviance and r2 are all very similar to model 3, meaning the extra term is not adding any useful information and hence should not be included.

```{r}
Data <- select(Data, -Rooms2)

# Model 5: Log transformation of Distance 
Data_log <- Data %>%
  mutate(Distance=ifelse(Distance == 0, 0.01, Distance)) %>% #log of 0 is negative infinity
  mutate(log_dis = log(Distance))
mod5 <- lm(Price~Bathroom+Rooms+Type+log_dis, data = Data_log)
glance(mod5)
```

This model results in a higher BIC and deviance than model 3, meaning it is not a better fit. The transformation also required altering data as log(0) is -infinity, so these values were changed to a small number (0.01) to give a real value. 

```{r}
# Model 6: Adding an interaction effect between distance and type. This may be useful as units and townhouses are typically more suitable closer to the CBD, hence each type may cause a different relationship between Distance and Price

mod6 <- lm(Price~Bathroom+Rooms+Distance*Type, data = Data)
glance(mod6)
```

This model has resulted in a slightly lower BIC and deviance values in comparison to model 3, suggesting the interaction effect provides a better model. While including the interaction does add more terms, the model is statistically better and there is an intuitive reason as to why. Because of this, mod6 is now the primary model. 

While the r2 value for this model is still quite low (0.43, indicating that 43% of variance in Price can be explained by this model), it was clear from the summary plots that there were no strong relationships between the selected variables and Price. This is, after all, real world data and there would be many other important factors that have not been considered (for example which suburb the property is in, distance to schools, shops, etc.). 

Considering the variables we have available, we have come to the conclusion that model 6 is the most suitable model discussed. Initially, we eliminated variables that were not providing benefit (Bedroom2). We then considered any potential transformations that became apparent from the summary plots (neither of which were useful). Finally we considered any real world interactions that would have an impact on the house Price. Through this iterative process we arrived at model 6.  This model had the lowest BIC and deviance of all models tested, indicating the greatest fit. 


```{r}
tidy(mod6)

final_model <- augment(mod6, Data)

ggplot(final_model) + 
  geom_point(aes(x=.fitted, y=.std.resid)) +
  xlab('Fitted Value') +
  ylab('Standard Residual')
```

The above graph plots the fitted value from the chosen model against the standard error. A good model would have no strong relationship between the 2. We can see a slight negative relationship between values with low residuals, however this relationship is likely too difficult to remove given the select data available. Unfortunately this plot also reveals a major issue with the model, with some fitted values falling below 0 (i.e. the model predicts a negative price which is obviously impossible.) This is a result of the negative distance term, which at large distances is greater than all other variables. This could be negated by transforming distance as with model 5.

```{r}
mod5 <- augment(mod5, Data_log)
ggplot(mod5) + 
  geom_point(aes(x=.fitted, y = .std.resid))
```

While there are less negative values using this model, they still occur when they shouldn't. The shape for both models is also similar, hence there is no reason to change to model 5. There is also no reason to remove the distance term itself, as the initial summary plot showed there is a relationship present. This can be further explored by looking at the residuals for Distance istelf.

```{r}
ggplot(final_model) + 
  geom_point(aes(x=Distance, y=.std.resid))
```

While there may be a slight positive correlation (and also greater variance at lower distances) the residuals are at large independent on distance, indicating the model does a good job of capturing the impact of Distance. 

Overall, while this model does have many drawbacks (mainly low r2 and negative price predictions), given the limited data, the model provides the best prediction available. Given the potential negative price predictions, the model should not be trusted at higher distances.

The final model can be written as:

`Price = 635273 + 238001*Bathrooms + 213297*Rooms-43076*Distance-571451*Townhouse-663581*Unit+28002*(Distance*Townhouse)+31513.41*(Distance*Unit)`
Where Rooms = # Rooms,
      Bathrooms = # Bathrooms,
      Townhouse = 1 if townhouse, 0 otherwise,
      Unit = 1 if unit, 0 otherwise,
      Distance = distance from CBD
