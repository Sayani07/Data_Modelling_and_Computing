---
title: 'Assignment 3'
output: html_document
---
```{r}
library(tidyverse)
library(visdat)
library(naniar)
library(impute)
library(broom)
houses <- read_csv("data/Melbourne_housing_FULL.csv")
houses <- select(houses,Price,Rooms,Type,Distance,Bedroom2,Bathroom)
vis_dat(houses, palette = "cb_safe")
```

#1.	From this graph we can find out that the data for type is the character, the data for Price; Rooms; Bedrooms; bedrooms2 and Bathrooms are the integer and the data of Distance is numeric. And we had also found out that there is some shadow on the colour lump of Price  Bedroom2 and Bathroom. That means we had the data missing on that variables. And the Bedroom2 have the same shadow with Bathroom. 
```{r}
vis_miss(houses, sort_miss=TRUE)
s_miss <- miss_summary(houses)
s_miss$miss_df_prop
s_miss$miss_var_prop
s_miss$miss_case_table
```

#2.	In this graph, we had shown clearly the missing of the different type of data. From the graph, we found out that the missing value of Bathroom and Bedroom 2 occupy 23.6%. And the missing value in Price accounted for 21.6%. There is no missing value in Distance, Rooms, and Type. We can also found out that the all the missing value is responsible for 11.5% of all data. And the present had taken up 88.5% of all data. 
```{r}
houses <- filter(houses,!is.na(houses$Distance))
houses$Bathroom[which(houses$Bathroom==0)] <- NA
```

#3.	Because there is a missing value in the distance, so we had to delete that. And as we all know that there is no living house in Australia can be with no bathroom. So we had to delete that data at first, and we want to put back later.
```{r}
h <- arrange(houses,Distance,Rooms,Price)
h <- select(h,Rooms,Distance,Price)
shadow <- bind_shadow(h)
for (i in c(1,2,3,4,5,6,7,8,9,10,12,16)){
h1 <- filter(h,Rooms==i)

ou1 <- impute.knn(as.matrix(h1),k=5)

shadow1 <- bind_shadow(h1) %>% mutate(Price=ou1$data[,3])

ifelse(i==1,shadow2 <- shadow1,shadow2 <- rbind(shadow2,shadow1))
}
```

#The variables ¡°Bedroom2¡± and¡± Bathroom¡± are likely to have missing value in the same line simultaneously, thus we add ¡°distance¡± as the proof of imputing price. That¡¯s why we use  ¡°Knn¡± method to impute ¡°Price¡± by ¡°Rooms¡±.

```{r}
ggplot(shadow2,
       aes(x = Distance,
           y = Price,colour=Price_NA)) +
  geom_point() + 
  scale_colour_brewer(palette="Dark2") +
  facet_wrap(~Rooms)
```

#As we can see from charts, ¡°Price¡± by imputing is comparatively good because it does not mix with any missing values.

```{r}
shadow2 <- arrange(shadow2,Distance,Rooms,Price)
houses2 <- arrange(houses,Distance,Rooms,Price) %>% mutate(Price=shadow2$Price)

shadow2 <- mutate(bind_shadow(houses2),Bathroom=as.integer(round(Rooms/2+0.25))) 
shadow2 <- mutate(shadow2,Price_NA=shadow$Price_NA) 
```

#Based on the common fact that almost 2 rooms are matched with 1 bathroom in Melbourne, we decide to impute the missing value of ¡°Bathroom¡± as a half of the missing value of ¡°Rooms¡±.To avoid ¡°bathroom¡± being zero, we add an value of 0.25 to the equation.


```{r}
ggplot(shadow2,
       aes(x = Rooms,
           y = Bathroom,colour=Bathroom_NA)) +
  geom_jitter() + 
  scale_colour_brewer(palette="Dark2")
```

#As we expected, the values of ¡°Bathroom¡± by imputing don¡¯t mix with any missing values.Then, we will explain the relationship between price and different variables. One important point to notice, according to our test, there are over 25000 cases that the value of ¡°Bedroom2¡± is equal to ¡°Rooms¡±. Except 8000 missing value of ¡°Bedroom2¡±, the number of that ¡°Bedroom2¡± is different to¡± Rooms¡± is 900, making up 2.6 percent of the sample. We can assure that, in fact, ¡°Rooms¡± represent the number of bedrooms. Besides, in some case the value of ¡°Bedroom2¡± is 0, which is not reasonable. Then we take sampling in the ¡°Bedroom¡± to compare it with its actual value and there are some errors with ¡°Rooms¡± in some cases.     In conclusion, we decide to keep ¡°Rooms¡± but abandon ¡°Bedroom2¡± because the value of ¡°Rooms¡± are complete and basically accurate.


```{r}
ggplot(shadow2,
       aes(x = Distance,
           y = Price,colour=Type)) +
  geom_point() +  
  facet_wrap(~Rooms)
```

#The facet-by-type graph is used to explain the relationship between price and distance. As we can see, the closer to CBD, the higher the price of house will be. Besides, ¡°Unit¡± usually have 1-3 bedrooms and ¡°Townhouse¡± usually have 2-5 bedrooms. For ¡°House¡±, it includes all combinations of bedroom. Moreover, many ¡°Units¡± are built within 20 kilometers of CBD. Most of townhouses are also within 20 kilometers. One interesting finding is that it seems that the peak of price appears at about 10 kilometers, which implies that houses with higher price are concentrated in areas such as Malvern, Brighton or Canterbury, just 10 kilometers.to the CBD. 

```{r}
ggplot(shadow2,
       aes(x = Rooms,
           y = Price,colour=Type)) +
  geom_point() +
  facet_wrap(~Type)
```

#The facet-by-type graph is used to explain the relationship between price and the number of rooms. The housing price basically follow the principle that the more bedrooms you get, the higher price you might get. It is easy to understand that housing price increases with the more bedrooms and larger size.

```{r}
model_1 <- lm(I(log(Price))~Distance+Type*Rooms+Bathroom-1, data=shadow2)
tidy(model_1)
glance(model_1)
confint_tidy(model_1)

p_coefs <- bind_cols(tidy(model_1), confint_tidy(model_1))
ggplot(p_coefs, aes(x=term, y=estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high))
```

#We decide to use the log-level model, because from the previous graphs we can see that many points of ¡°Price¡± are intensively on the left side of the graph, and the result of using the level-level regression model would not be ideal (we have tested and compared with the two models to verify this idea.). Then we add the relevant variables. One point to notice is that we have a few assumptions in the part of ¡°Interaction term¡± like ¡°Type and Rooms¡±, ¡°Distance and Rooms¡± and ¡°Type and Distance¡±. Each assumption has their own reasonable part. Finally, we decide ¡°Type and Rooms¡± as the best ¡°Interaction term¡± by test. It is because that ¡°Unit¡± are usually located close to the city center, and one more bedroom will have a greater impact on housing price than ¡°Townhouse¡± and ¡°House¡±. Moreover, each independent variable of this model is significant.

```{r}
mod1 <- augment(model_1,shadow2)

ggplot(mod1, aes(x=Distance)) + 
  geom_point(aes(y=exp(.fitted)))+
  facet_wrap(~Rooms)
 
ggplot(mod1, aes(x=.fitted, y=log(Price))) +
  geom_point() 

ggplot(mod1, aes(x=.fitted, y=.std.resid)) +
  geom_point()
```

#As we can see, basically, the predicted values of the model are close to its actual values. However, there are a lot of irregular points when .fitted is larger than 15. We find that only 269 cases exist when the number of ¡°Room¡± is larger than 6, because of which the result of the regression is not exact. Thus, we want to know what the results we will get when we remove these cases.
```{r}
shadow2 <- filter(shadow2,Rooms<=5)

model_1 <- lm(I(log(Price))~Distance+Type*Rooms+Bathroom-1, data=shadow2)
tidy(model_1)
glance(model_1)
confint_tidy(model_1)

p_coefs <- bind_cols(tidy(model_1), confint_tidy(model_1))
ggplot(p_coefs, aes(x=term, y=estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high))

mod1 <- augment(model_1,shadow2)

ggplot(mod1, aes(x=Distance)) + 
  geom_point(aes(y=exp(.fitted)))+
  facet_wrap(~Rooms)
 
ggplot(mod1, aes(x=.fitted, y=log(Price))) +
  geom_point() 

ggplot(mod1, aes(x=.fitted, y=.std.resid)) +
  geom_point()
```

#It is obvious that now we get more satisfied results¡ª¡ªholding other independent variables significant, t we could get, the he higher r.squared, higher adj.r.squared, lower AIC, lower BIC and lower deviance.As a result, the r.squared of this model is 0.9993557. Adj.r.squared is 0.9993555. AIC is 25363.64. BIC is 25439.7 and deviance is 421.37986. All of the values are the best among the models we tested, and we can easily see that std.resid has a quite good distribution. So, we think this model is the best.
