---
title: "Assignment 3"
author: "Kat and Tim"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Missing Values Summary

```{r}
library(tidyverse)
library(visdat)
library(naniar)
library(broom)
housing <- read.csv("data/Melbourne_housing_FULL.csv") %>%
  select(Price, Rooms, Type, Distance, Bedroom2, Bathroom) %>%
  mutate(Type = ifelse(Type == "u", "Unit", ifelse(Type == "h", "House", ifelse(Type == "t", "Townhouse", NA)))) %>%
  mutate(Distance = as.character(Distance)) %>% mutate(Distance = as.numeric(Distance)) %>%
  mutate(Bedroom = as.integer(Bedroom2)) %>%
  select(-Bedroom2)
vis_dat(housing)
miss_summary(housing)
housing_shadow <- bind_shadow(housing)
```

The summary of missing values indicates that around 40% of the cases are missing values for one or more variables, and 11.5% of the total observations are missing. It also shows that two thirds of the variables contain missing values, with the table confirming that Price, Bathroom and Bedroom are the variables with significant missing values, meaning we need to impute values for these variables. One option is to impute the values using the means or medians of the variables, however we can likely make a better prediction of the missing values using the relationship between Bedroom/Bathroom and the other variables in the dataset. Therefore, we are imputing the missing values for Bathroom and Bedroom using a linear regression model of these variables on the other variables in the dataset. We do not include Bathroom in the model for Bedroom and vice-versa, as the table shows that the cases missing values for Bedroom are also missing values for Bathroom, meaning we will not be able to predict missing value of one from the other.

```{r}
bathroom_imputation <- lm(housing, formula=Bathroom ~ Price + Rooms + Type + Distance)
summary(bathroom_imputation)
bedroom_imputation <- lm(housing, formula=Bedroom ~ Price + Rooms + Type + Distance)
summary(bedroom_imputation)
```

The summaries of these models indicate that all other variables in the dataset are significant, and as such these are the models we will use to impute the missing values.

```{r}
summary(predict(bathroom_imputation))
summary(predict(bedroom_imputation))
summary(housing)
housing_shadow <- housing_shadow %>% 
  mutate(Bathroom = ifelse(is.na(Bathroom),round(predict(bathroom_imputation), digits=0),  Bathroom)) %>%
  mutate(Bedroom = ifelse(is.na(Bedroom),round(predict(bedroom_imputation), digits=0),  Bedroom)) %>%
  filter(complete.cases(Distance))
ggplot(housing_shadow, aes(x=Price,y=Bedroom,colour=Bedroom_NA,alpha=Bedroom)) +
  geom_point() +
  coord_cartesian(ylim=c(0,15)) +
  facet_wrap(~Bedroom_NA)
ggplot(housing_shadow, aes(x=Price,y=Bathroom,colour=Bathroom_NA,alpha=Bathroom)) +
  geom_point() +
  coord_cartesian(ylim=c(0,10)) +
  facet_wrap(~Bathroom_NA)
```

Furthermore, the quartiles and means of the imputed values are not significantly dissimilar to those of the observations on these variables except that the maximums of the imputed values are significantly lower. The plots of the values also show that the impute values appear to fit in with the observed data without any outliers or irregular patterns.

```{r}
vis_dat(housing_shadow)
miss_summary(housing_shadow)
```

After removing the one missing observation on Distance, the only variable containing missings is now Price, with around 22% of cases missing an observation on Price. As price is the explanatory variable, we do not impute its missing values using a linear regression model of the other variables. Instead, since number of rooms is a significant indicator of price, we assign missing values the mean of price conditional on the number of rooms.

```{r}
r1 <- bind_shadow(housing) %>%
  filter(Rooms==1) %>%
  mutate(Price=ifelse(is.na(Price),mean(Price,na.rm=TRUE),Price))
r2 <- bind_shadow(housing) %>%
  filter(Rooms==2) %>%
  mutate(Price=ifelse(is.na(Price),mean(Price,na.rm=TRUE),Price))
r3 <- bind_shadow(housing) %>%
  filter(Rooms==3) %>%
  mutate(Price=ifelse(is.na(Price),mean(Price,na.rm=TRUE),Price))
r4 <- bind_shadow(housing) %>%
  filter(Rooms==4) %>%
  mutate(Price=ifelse(is.na(Price),mean(Price,na.rm=TRUE),Price))
r5 <- bind_shadow(housing) %>%
  filter(Rooms==5) %>%
  mutate(Price=ifelse(is.na(Price),mean(Price,na.rm=TRUE),Price))
r6 <- bind_shadow(housing) %>%
  filter(Rooms==6) %>%
  mutate(Price=ifelse(is.na(Price),mean(Price,na.rm=TRUE),Price))
r7plus <- bind_shadow(housing) %>%
  filter(Rooms>7) %>%
  mutate(Price=ifelse(is.na(Price),mean(Price,na.rm=TRUE),Price))
housing_shadow <- bind_rows(r1,r2,r3,r4,r5,r6,r7plus) %>% 
  mutate(Bathroom = ifelse(is.na(Bathroom),round(predict(bathroom_imputation), digits=0),  Bathroom)) %>%
  mutate(Bedroom = ifelse(is.na(Bedroom),round(predict(bedroom_imputation), digits=0),  Bedroom)) %>%
  filter(complete.cases(Distance))
ggplot(housing_shadow,aes(x=Rooms,y=Price,colour=Price_NA,alpha="identity")) +
  geom_point()
vis_dat(housing_shadow)
miss_summary(housing_shadow)
housing <- housing_shadow %>% select(Price,Rooms,Type,Distance,Bathroom,Bedroom)
```

We see that the missings fit the existing data reasonably well and that now all missing values have been eliminated

####Price and Number of Rooms
```{r}
ggplot(housing,aes(Rooms)) +
  scale_x_continuous(breaks = seq(0, 16, by = 1)) +
  geom_histogram(binwidth=0.5)
ggplot(housing %>% filter(Rooms<7), aes(x=Rooms,y=Price)) +
  geom_boxplot(aes(group=Rooms,alpha="identity")) +
  stat_smooth(method=lm,colour="red")
```

For the variables involving number of rooms, we use boxplots as these variables are discrete rather than continuous, and with such a large number of observations boxplots give us a better indication of the density of the data. However, boxplots can be misleading in small sample sizes. Looking at the distribution of Rooms we then see that there are very few instances of Rooms greater than 6, meaning that these values may be inconsistent with the overall pattern due to such small sample sizes. Therefore, for the boxplot we plot only cases where the value of Rooms is less than 7, and we see a positive correlation between number of rooms and Price.

####Price and Number of Bedrooms
```{r}
ggplot(housing,aes(Bedroom)) +
  scale_x_continuous(breaks = seq(0, 16, by = 1)) +
  coord_cartesian(xlim=c(0,16)) +
  geom_histogram(binwidth=0.5)
ggplot(housing %>% filter(Bedroom<7,Bedroom>0), aes(x=Bedroom,y=Price)) +
  geom_boxplot(aes(group=Bedroom,alpha="identity")) +
  stat_smooth(method=lm,colour="red")
```

We see a similar pattern with the distribution of bedrooms, with there being very few cases of Bedroom being greater than 6. For the same reason we again only include cases where Bedroom is less than 7 in the boxplots, and again see a positive relationship between number of bedrooms and Price. 

####Price and Number of Bathrooms
```{r}
ggplot(housing,aes(Bathroom)) +
  scale_x_continuous(breaks = seq(0, 8, by = 1)) +
  coord_cartesian(xlim=c(0,8)) +
  geom_histogram(binwidth=0.5)
ggplot(housing %>% filter(Bathroom<7,Bathroom>0), aes(x=Bathroom,y=Price)) +
  geom_boxplot(aes(group=Bathroom,alpha="identity")) +
  stat_smooth(method=lm,colour="red")
```

We see a similar pattern with the distribution of bathrooms, with there being very few cases of Bathroom being greater than 6. For the same reason we again only include cases where Bathroom is less than 7 in the boxplots, and again see a positive relationship between number of bathrooms and Price.              

When building a model which will best explain our data, we choose to remove Rooms as a regressor but keep both Bedrooms and Bathrooms. This is because multicolinearity exists between Rooms and Bathrooms as well as Rooms and Bedrooms. Keeping Bedrooms and Bathrooms as explanatory variables will allow for a more detailed model as opposed to choosing to just keep Rooms. Below, we examine the effect of each x variable on our y variable and obtain the best fitted model: the equation for which will form part of our final model.Note: we are using these variables after imputing to deal with the NAs. 

We want to determine which variables to include in our model. To minimize uncertainty in parameter estimates, associations between explanatory variables should be minimized. We examine the strength of the linear relationships between our explanatory variables using correlation:
```{r}
BathBedCorr <- cor(housing$Bedroom, housing$Bathroom)
RoomsBedCorr <- cor(housing$Bedroom,housing$Rooms)
DistanceBedCorr <- cor(housing$Bedroom,housing$Distance)

RoomsBathCorr <- cor(housing$Bathroom, housing$Rooms)
DistanceBathCorr <- cor(housing$Bathroom, housing$Distance)

DistanceRoomsCorr <- cor(housing$Rooms, housing$Distance)

Correlation_values <- c(BathBedCorr, RoomsBathCorr, DistanceBathCorr, RoomsBathCorr, DistanceBathCorr, DistanceRoomsCorr)
Correlation_values
```

From the correlation values it can be noted that there is no extremely strong linear correlation that exists between explanatory variables to warrant the removal of any of the variables Bathroom, Bedroom, Distance or Rooms from the model. Note: a correlation analysis was not conducted on 'Type' as it is a categorical variable and using economic theory; its importance and hence inclusion in the following model is justifiable. Subsequently we have chosen to maintain all variables in our model. 

Next we plot scatterplots of our continuous explanatory variable against price to get an idea of the type of distribution or family of functions which is likely to be the most appropriate. We use geom_smooth to help us visualise the best curve/ fit. (Note: limits have been selected based on what can be considered reasonable and in order to allow geom_smooth to run). 
```{r}
ggplot(housing, aes(x=housing$Bedroom,y=housing$Price)) +
  geom_point(colour='red')+ geom_smooth(se=FALSE)+scale_x_continuous(limits = c(0, 15))

ggplot(housing, aes(x=housing$Bathroom,y=housing$Price)) +
  geom_point(colour='orange')+ geom_smooth(se=FALSE)+scale_x_continuous(limits = c(0, 9))

ggplot(housing, aes(x=housing$Rooms,y=housing$Price)) +
  geom_point(colour='green')+ geom_smooth(se=FALSE)+scale_x_continuous(limits = c(0, 15))


```

##Models
We trial a series of different models starting with a level- level model:
```{r}
mod1 <- lm(data = housing, Price~Rooms+Bedroom+Bathroom+Distance)
tidy(mod1)
glance(mod1)
```

With an R squared value of only 0.38 it is likely that we can find a much better model which explains more of our data. According to all our p-values being <0.05 this suggests that all the included variables are of significance. However, some of the coefficients of our estimates are different to what we would expect with 'Bedroom' suggesting that for a house possessing 1 more bedroom, one could expect the house price to drop by $34,673. This estimate may be a result of both Rooms and Bedrooms being included in the model and sharing some linearity because an increase in #bedroom of 1 leads to an increase in #room of 1. Therefore in the next model we exclude the Room variable: 

```{r}
mod2 <- lm(data= housing, formula= Price~ Bedroom+ Bathroom +Distance)
tidy(mod2)
glance(mod2)
```

With this model again all variables are deemed statistically significant and the regressor coefficients appear a lot more realistic, although still an overrepresentation. Given the magnitude of the coefficents the standard errors are also relatively small being approximately 2-3% the size of the coefficients implying tight confidence intervals. A major negative trade off is that the R squared value has decreased significantly to 0.23 emphasizing the significant impact the number of rooms has on house price leading us to believe that it would be better to continue to include this variable. Subsequently we next run a model including all variables as well as Type which will act as a dummy variable of dwelling type which we previously didn't include:  
```{r}
mod3 <- lm(data= housing, formula= Price~ Rooms + Bedroom + Bathroom + Distance + Type)
tidy(mod3)
glance(mod3)
```

Compared to model 1, the R squared value of model 3 has improved slightly from 0.38 in model 1 to 0.40. In order to determine whether the addition of this extra variable was meaningful (keeping in mind that additional variables will always lead to an increase in R squared), we use the adjusted R squared value as a fairer method of comparison. The adjusted R squared value in model 1 is 0.38 and 0.40 in model 3 - the same as R squared so we can conclude that it is beneficial including the Type variable in our model. After observing the scatterplots of the relationships between the individual x variables and y it appears that a log- level model will likely be a better fit for the data than a level- level model so we try this next: 
```{r}
ggplot(housing,aes(Price)) +
    geom_histogram(binwidth=250000)
mod4 <- lm(data=housing, formula=log(Price) ~ Rooms+Bathroom+Bedroom+Distance+Type)
tidy(mod4)
glance(mod4)
```

From the histogram of the Price variable the distribution appears to be significantly positively skewed, suggesting that a log transformation may be appropriate.
With this log-level model the R squared and adjusted R squared value have both increased to 0.50 which is the best we have seen thus far. All variables are deemed significant upon assessing the t-stat/ p-value but note the coefficients look very different to the coefficients in the previous models. With a log- level model we interpret the coefficients as signifing the effect of a one unit increase in x on the percentage increase or decrease in the variable y. For example, this model is suggesting that one additional bathroom will lead to an 8.7% increase in house price. We continue to use log in the following models but also consider the interactions which exist between the explanatory variables: 
```{r}
mod5 <- lm(data=housing, log(Price) ~ Rooms+Bathroom+Bedroom+Distance*Type)
tidy(mod5)
glance(mod5)
```

##Residuals
```{r}
ggplot(data=mod1, aes(x=Price,y=residuals(mod1),alpha="identity")) +
  geom_point()
ggplot(data=mod2, aes(x=Price,y=residuals(mod2),alpha="identity")) +
  geom_point()
ggplot(data=mod3, aes(x=Price,y=residuals(mod3),alpha="identity")) +
  geom_point()
ggplot(data=mod4, aes(x=`log(Price)`,y=residuals(mod4),alpha="identity")) +
  geom_point()
```

We see from the plot of the residuals that while there does still appear to be some structure in the residuals, the pattern is significantly reduced compared to the residuals in models 1 to 3. We also see that in particular the extreme residuals seen in models 1 to 3 have been eliminated and that models 4 and 5 produces the residuals that are closest to appearing random, although they are not without structure. This further

##Conclusion
Model 5 produces the best result out of all which we have experimented with emphasizing the importance of considering the interactions which exist between explanatory variables and the need to account for this. The interaction terms allow for the impact of a change in distance to vary depending on the type of house. One interpretation of this is that for people purchasing Units proximity to the city may be more important for work purposes, compared to townhouses where it is likely that proximity to the city is less important. Model 5 It maximises R squared and adjusted R squared, and minimises AIC, BIC and deviance compared to the other models we have experimented with. Therefore in summary our final model is: 
log(Price)= 
13.36 + 0.24Rooms + 0.09Bathroom - 0.02Bedroom - 0.03Distance - 0.32Townhouse - 0.53Unit + 0.02(Distance * Townhouse) + 0.02(Distance * Unit)
