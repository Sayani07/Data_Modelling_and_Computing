---
title: "ETC1010: Data Modelling and Computing"
output: 
  learnr::tutorial:
    css: "css/logo.css"
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = TRUE,   
                      message = FALSE,
                      warning = FALSE,
                      collapse = TRUE,
                      fig.height = 6,
                      fig.width = 6,
                      fig.align = "center",
                      cache = FALSE)
tutorial_html_dependency()
library(tidyverse)
library(plotly)
```

# Text analysis

## Course web site

This is a link to the course web site, in case you need to go back and forth between tutorial and web materials: [http://dmac.netlify.com](http://dmac.netlify.com)

## Overview

<img src="images/text_analysis.png" height=300>

### Why?

- To use the realtors text description to improve the Melbourne housing price model
- Determine the extent of public discontent with train stoppages in Melbourne
- The differences between Darwin's first edition of the Origin of the Species and the 6th edition
- Does the sentiment of posts on Newcastle Jets public facebook page reflect their win/los record?

### Process

1. read in text
2. pre-processing: remove punctuation signs, remove numbers, stop words, stem words
3. tokenise: words, sentences, ngrams, chapters
4. summarise, model

### Resource

[Text Mining with R by Julia Silge and David Robinson](https://www.tidytextmining.com)

## Example: The Origin of the Species

The [Gutenberg project](http://www.gutenberg.org/wiki/Main_Page) provides the text of over 57000 books free online. 

Navigate to The Origin of the Species by Charles Darwin. There are two versions available. 

You can either download the text directly or use the `gutenbergr` package. To use the package you need to know the `id` of the book, which means looking this up online anyway. (The first edition is `1228`, and the 6th edition is `2009`)

```{r}
# install.packages("tidytext")
library(tidytext)
# The tm package is needed because the book has numbers 
# in the text, that need to be removed, and the
# tidytext package doesn't have a function to do that
# install.packages("tm")
library(tm)
# install.packages("gutenbergr")
library(gutenbergr)
darwin1 <- gutenberg_download(1228)
darwin1
```

### Tokenize

```{r}
darwin1$text <- removeNumbers(darwin1$text)
darwin1_words <- darwin1 %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, sort=TRUE) %>%
  mutate(len = str_length(word)) 
quantile(darwin1_words$n, probs = seq(0.9, 1, 0.01))
darwin1_words %>% filter(n > 90) %>%
  ggplot(aes(x=n, y=fct_reorder(word, n))) + geom_point() +
  ylab("")
```

### Your turn

Download and tokenize the 6th edition.

```{r results='hide', fig.show='hide'}
darwin6 <- gutenberg_download(2009)
darwin6$text <- removeNumbers(darwin6$text)
darwin6_words <- darwin6 %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, sort=TRUE) %>%
  mutate(len = str_length(word)) 
quantile(darwin6_words$n, probs = seq(0.9, 1, 0.01))
darwin6_words %>% filter(n > 90) %>%
  ggplot(aes(x=n, y=fct_reorder(word, n))) + geom_point() +
  ylab("")
```

### Compare the word frequency

```{r}
library(plotly)
darwin <- full_join(darwin1_words, darwin6_words, by = "word") %>%
  rename(n_ed1 = n.x, len_ed1 = len.x, n_ed6 = n.y, len_ed6 = len.y)
p <- ggplot(darwin, aes(x=n_ed1, y=n_ed6, label=word)) + 
  geom_abline(intercept=0, slope = 1) +
  geom_point(alpha=0.5) +
  xlab("First edition") + ylab("6th edition") +
  scale_x_log10() + scale_y_log10() + theme(aspect.ratio=1)
ggplotly(p)
```

### Your turn

- Does it look like the 6th edition was an expanded version of the first?
- What word is most frequent in both editions?
- Find some words that are not in the first edition but appear in the 6th.
- Find some words that are used the first edition but not in the 6th.
- Using a linear regression model find the top few words that appear more often than expected, based on the frequency in the first edition. Find the top few words that appear less often than expected. 

```{r eval=FALSE, echo=FALSE}
# This code is a guide, but you need to think about transformations
# and perhaps relative increase, or filtering outliers
library(broom)
mod <- lm(n_ed6~n_ed1, data=darwin)
tidy(mod)
fit <- augment(mod, darwin)
ggplot(fit, aes(x=n_ed1, y=.resid)) + geom_point(alpha=0.5)
fit %>% arrange(desc(.resid)) %>% top_n(10)
```

### Book comparison

The idea is to find the important words for the content of each document by decreasing the weight of commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.

The statistic *term frequency, inverse document frequency*, `tf-idf`, is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.

$tf_{word} = \frac{Number~of~times~word~t~appears~in~a~document}{Total~number~of~words~in~the~document}$

$idf_{word} = log \frac{number~of~documents}{number~of~documents~word~appears~in}$

$td-idf = tf\times idf$

```{r}
darwin1_words <- darwin1_words %>%
  mutate(edition = "1")
darwin6_words <- darwin6_words %>%
  mutate(edition = "6")
darwin <- bind_rows(darwin1_words, darwin6_words)
darwin <- darwin %>% bind_tf_idf(word, edition, n)
darwin %>% arrange(desc(tf_idf))
darwin %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(edition) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = edition)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~edition, ncol = 2, scales = "free") +
  coord_flip() + scale_fill_brewer(palette="Dark2")
```

What do we learn?

- Mr Mivart appears in the 6th edition, multiple times
```{r}
grep("Mivart", darwin6$text)
darwin6[5435,]
```
- Prof title is used more often in the 6th edition
- There is a tendency for latin names 
- Mistletoe was mispelled in the 1st edition

### Your turn

Text Mining with R has an example comparing historical physics textbooks:  *Discourse on Floating Bodies* by Galileo Galilei, *Treatise on Light* by Christiaan Huygens, *Experiments with Alternate Currents of High Potential and High Frequency* by Nikola Tesla, and *Relativity: The Special and General Theory* by Albert Einstein. All are available on the Gutenberg project. 

Work your way through the [comparison of physics books](https://www.tidytextmining.com/tfidf.html#a-corpus-of-physics-texts). It is section 3.4.

## Sentiment analysis

Sentiment analysis tags words or phrases with an emotion, and summarises these, often as the positive or negative state, over a body of text. 

Examples of use are:

- Examining effect of emotional state in twitter posts
- Determining public reactions to government policy, or new product releases
- Trying to make money in the stock market by modeling social media posts on listed companies
- Evaluating product reviews on Amazon, restaurants on zomato, or travel options on TripAdvisor

The `tidytext` package has a lexicon of sentiments, based on four major sources: [AFINN](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010), [bing](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), [Loughran](https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists), [nrc](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)

```{r}
sentiments %>% count(sentiment, sort=TRUE)
```

### Example: Simpsons

Data from the popular animated TV series, The Simpsons, has been made available on [kaggle](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data/data). 

- `simpsons_script_lines.csv`: Contains the text spoken during each episode (including details about which character said it and where)
- `simpsons_characters.csv`: Contains character names and a character id

```{r}
scripts <- read_csv("data/simpsons_script_lines.csv")
chs <- read_csv("data/simpsons_characters.csv")
sc <- left_join(scripts, chs, by = c("character_id" = "id") )
sc %>% count(name, sort=TRUE)
```

#### Pre-process the text

```{r}
sc %>%
  unnest_tokens(word, normalized_text) %>%
  filter(!word %in% stop_words$word) %>%
  count(word, sort = TRUE) %>%
  ungroup() %>%
  filter(!is.na(word)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  head(20) %>%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat='identity', colour="white") +
  labs(x = '', y = 'count', 
       title = 'Top 20 words') +
  coord_flip() + 
  theme_bw()
```

#### Tag the words with sentiments

Using AFINN words will be tagged on a negative to positive scale of -1 to 5.

```{r}
sc_word <- sc %>%
  unnest_tokens(word, normalized_text) %>%
  filter(!word %in% stop_words$word) %>%
  count(name, word) %>%
  ungroup() %>%
  filter(!is.na(word))
sc_s <- sc_word %>% 
  inner_join(get_sentiments("afinn"), by = "word")
```

#### Examine characters

```{r}
sc_s %>% group_by(name) %>% 
  summarise(m=mean(score)) %>% 
  arrange(desc(m))
```

Oh, maybe we want to only focus on the main characters.

```{r}
keep <- sc %>% count(name, sort=TRUE) %>%
  filter(!is.na(name)) %>%
  filter(n > 999)
sc_s %>% filter(name %in% keep$name) %>% 
  group_by(name) %>% 
  summarise(m=mean(score)) %>% 
  arrange(m)
```

#### Your turn

1. Bart Simpson is featured at various ages. How has the sentiment of his words changed over his life?

```{r eval=FALSE}
sc %>% count(name, sort=TRUE) %>%
  filter(grepl("Bart", name)) %>% print(n=50)
bart_names <- c("Bart Simpson", "Baby Bart", 
                "1-Year-Old Bart", "2-Year-Old Bart", 
                "5-Year-Old Bart", "80-Year-Old Bart")
bart <- sc %>% filter(name %in% bart_names)
bart_word <- bart %>%
  unnest_tokens(word, normalized_text) %>%
  filter(!word %in% stop_words$word) %>%
  count(name, word) %>%
  ungroup() %>%
  filter(!is.na(word))
bart_s <- bart_word %>% 
  inner_join(get_sentiments("afinn"), by = "word")
bart_s %>% group_by(name) %>% 
  summarise(m=mean(score)) %>% 
  arrange(desc(m))
```

2. Repeat the sentiment analysis with the NRC lexicon. What character is the most "angry"? "joyful"?

```{r eval=FALSE}
sc_s <- sc_word %>% 
  inner_join(get_sentiments("nrc"), by = "word")
statmode = function(x){ 
    ta = table(x)
    tam = max(ta)
    if (all(ta == tam))
         mod = NA
    else
         if(is.numeric(x))
    mod = as.numeric(names(ta)[ta == tam])
    else
         mod = names(ta)[ta == tam]
    return(mod)
}
sc_s %>% filter(name %in% keep$name) %>% 
  group_by(name) %>% 
  summarise(angry = sum(n[sentiment=="anger"]/sum(n))) %>%
  arrange(desc(angry))
```

## Sentiment of facebook posts

If you have a facebook account, you can get a token to automatically extract public posts, using the package `Rfacebook`.

1. Go to [facebook for developers](https://developers.facebook.com/tools/explorer/?method=GET&path=me%3Ffields%3Did%2Cname&version=v3.0)
2. Click on "Get token", "Get Page Access Token" and this will give you a temporary code to use to extract data

The code below was used to extract all the public posts from the "Newcastle Jets" facebook account. The Newcastle Jets just played the finals over the weekend, and lost after a referee error. 

```{r eval=FALSE}
library(Rfacebook)
token <- "EAACEdEose0cBABZAXeZCfIwsZAJsBAL9drBBNtMs2go831XnEDMJZBK6tlui0O2ZBjm5snq7E9QVjVuETHmjnNrAG7Qp1Wu2WALrqtoiFo2PQ5T75b4zvEZAH2PaPcMXoyjlqLEiNJJdDHaGAIKec9PZAL2oqFzPSlOojBZBEWvjgHKxOl5qHqkW5kynsqavQZCWIBuVbHSqzTgZDZD"
ncj <- "newcastlejetsfc"
ncj_posts <- getPage(ncj, token, n=1000, feed = TRUE)
save(ncj_posts, file="data/ncj_posts.rda")
```

Let's take a look at the average sentiment score of posts by week. And compare them with win-loss record for the team. 

```{r }
library(lubridate)
load("data/ncj_posts.rda")
msg_word <- ncj_posts %>% 
  group_by(id) %>% 
  unnest_tokens(word, message) %>%
  anti_join(stop_words) %>%
  count(word, sort=TRUE) %>%
  inner_join(get_sentiments("afinn"), by = "word")
# Compute average sentiment score
msg_word_s <- msg_word %>% 
  group_by(id) %>%
  summarise(score = mean(score)) 
ncj_posts <- ncj_posts %>% 
  mutate(date = date(created_time)) %>%
  mutate(wday = wday(date, label = TRUE, abbr = TRUE, week_start=1))
msg_word_s <- msg_word_s %>%
  left_join(ncj_posts, by="id")
msg_word_s_17_18 <- msg_word_s %>% filter(date > ymd("2017-10-01")) %>%
  mutate(week = week(date)) %>%
  mutate(week = ifelse(week < 19, week+53, week)) %>%
  mutate(week = week - 40)
msg_word_s_17_18_av <- msg_word_s_17_18 %>%
  filter(type == "photo") %>%
  group_by(week) %>%
  summarise(m = mean(score, na.rm=TRUE))
```

```{r fig.width=5, fig.height=2}
# Read in match scores
ncj_scores <- read_csv("data/ncj.csv")
ncj_scores$win <- "W"
ncj_scores$win[ncj_scores$team1 == "Newcastle Jets"&ncj_scores$score1<ncj_scores$score2] <- "L"
ncj_scores$win[ncj_scores$team2 == "Newcastle Jets"&ncj_scores$score2<ncj_scores$score1] <- "L"
ncj_scores$win[ncj_scores$score1==ncj_scores$score2] <- "D"
ncj_scores <- ncj_scores %>% mutate(date = dmy(date)) %>%
  mutate(week = week(date)) %>%
  mutate(week = ifelse(week < 19, week+53, week)) %>%
  mutate(week = week - 40)
ggplot() +
  geom_line(data=msg_word_s_17_18_av, aes(x=week, y=m)) +
  geom_text(data=ncj_scores, aes(x=week, y=-2, label=win, colour=win)) +
  scale_colour_brewer(palette="Dark2")
```

## Share and share alike

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
