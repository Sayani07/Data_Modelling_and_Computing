---
title: "Introduction"
author: "Di Cook"
output: 
  learnr::tutorial:
    css: "css/logo.css"
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE,   
                      message = FALSE,
                      warning = FALSE,
                      collapse = TRUE,
                      fig.height = 4,
                      fig.width = 8,
                      fig.align = "center",
                      cache = FALSE)
tutorial_html_dependency()
```

## Welcome!

Lovely to see your new shining faces, eager for the new year.

- I'm Di, a professor at Monash University in Melbourne Australia, and I like to play all sorts of sports, tennis, soccer, hockey, cricket, and go boogie boarding.
- This is Steph! She is the lab manager for the course. This means that she will help solve problems and manage operations during the lectorial, but will not grade. She just finished her undergraduate degree, and will start a masters in Statistics soon.
- This is Stuart! Stuart is a second year PhD student in Econometrics and Business Statistics. Stuart is interested in biological data, big, big data, and especially visualisation of the data.
- This is Earo! Earo is a third year PhD student. She graduated with Bachelor of Commerce, Honours I from Monash. Her expertise is in all sorts of data wrangling and visualisation, especially for time series. 

```{r}
library(tidyverse)
library(gridExtra)
library(plotly)
library(ggthemes)
```

### Exercise 

You best resources can be right beside you!

- Introduce yourself to the other class mates at your table. 
- Tell everyone at the table one thing that is interesting about yourself. 
- Try to make up a (long) name for your table, that summarises everyone's interests.

## Materials

- __Unit guide__: Objectives, tentative schedule, grading
- __Textbook__: [R for Data Science](http://r4ds.had.co.nz), Garret Grolemund and Hadley Wickham
- __Computing__: R and RStudio
- __Materials__: Moodle for grades; [course web site](https://dmac.netlify.com) for lecture and lab materials; online quizzes, lab and assignment submissions [ED](https://edstem.org/)

## Learning objectives

- apply principles and techniques of data management with computers and spreadsheet modelling to business and economic decision-making
- interpret and evaluate relationships between variables using simple and multiple linear regression
- apply statistical techniques for making decisions with quantitative and categorical data in business and economics

## Philosophy

"If you feed a person a fish, they eat for a day. If you teach a person to fish, they eat for a lifetime."

Whatever I do, you can do too.

## Quiz

```{r operators}
quiz(
  question("Where do I find the class lectorial notes?",
    answer("moodle"),
    answer("https://edstem.org/"),
    answer("class web site (http://dmac.netlify.com)", correct = TRUE),
    answer("http://r4ds.had.co.nz")),
  question("Where do I find my grades?",
    answer("moodle", correct = TRUE),
    answer("https://edstem.org/"),
    answer("class web site (http://dmac.netlify.com)"),
    answer("http://r4ds.had.co.nz")),
  question("What is the link for the course textbook?",
    answer("moodle"),
    answer("https://edstem.org/"),
    answer("class web site (http://dmac.netlify.com)"),
    answer("http://r4ds.had.co.nz", correct = TRUE))
)
```
  
## Outline

- What is data? 
- What can we do if we have good data handling skills?
    - __Insert lots of examples here__
- Why R?

## This course

__Data preparation accounts for about 80% of the work of data scientists__

[Gil Press, Forbes, 2016](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/##47cbbbf46f63)

This is one of the least taught parts of data science, and business analytics, and yet it is what data scientists spend most of their time on. By the end of this semester, we hope that you will have the tools to be more efficient and effective in this area, so that you have more time to spend on your mining and modeling.

## Music

### Reading audio data

- Read in a wave file
- Digitise it as time and amplitude
- Plot
- Compare with other sounds

```{r}
library(tuneR)
m1 <- readWave("data/data3.wav")
m1 <- extractWave(m1, from = 25000, to = 75000)
m1_df <- data.frame(t=1:length(m1@left), 
                    left=m1@left, 
                    right=m1@right)
p1 <- ggplot(m1_df, aes(x=t, y=left)) + geom_line() + ggtitle("Say `data`")
m2 <- readWave("data/statistics1.wav")
m2 <- extractWave(m2, from = 25000, to = 75000)
m2_df <- data.frame(t=1:length(m2@left), 
                    left=m2@left, 
                    right=m2@right)
p2 <- ggplot(m2_df, aes(x=t, y=left)) + geom_line() + ggtitle("Say `statistics`")
grid.arrange(p1, p2, ncol=1)
```

```{r audio}
quiz(
  question("How does `data` appear different than `statistics` in the time series?",
    answer("statistics is longer"),
    answer("statistics has more peaks", correct=TRUE),
    answer("they are the same")),
  question("What format is the data in an audio file?",
    answer("binary", correct=TRUE),
    answer("text"),
    answer("comma separated"),
    answer("tab separated"),
    answer("JSON"))
)
```

### Compare left and right channels

```{r}
p1 <- ggplot(m1_df, aes(x=left, y=right)) + 
  geom_point() + theme(aspect.ratio=1)
p2 <- ggplot(m2_df, aes(x=left, y=right)) + 
  geom_point() + theme(aspect.ratio=1)
grid.arrange(p1, p2, ncol=2)
```

Oh, same sound is on both channels! A tad drab.

### Compute statistics

```{r}
m1_df <- m1_df %>% gather(channel, value, left, right) %>%
  mutate(word="data")
m2_df <- m2_df %>% gather(channel, value, left, right) %>%
  mutate(word="statistics")
m_df <- bind_rows(m1_df, m2_df)
m_df %>% filter(channel == "left") %>%
  group_by(word) %>%
  summarise(m = mean(value), s = sd(value), 
            mx = max(value), mn = min(value))
```

### My music - please don't laugh

```{r}
music <- read.csv("data/music-sub.csv", 
                  row.names=1, stringsAsFactors = FALSE)
ggplot(music, aes(x=artist, y=lave)) + geom_boxplot() +
  xlab("") + ylab("Average amplitude")
```

```{r fig.height=4, fig.weight=4}
ggplot(music, aes(x=lvar, y=lave, colour=artist)) + 
  geom_point(size=5, alpha=0.5) +
  scale_colour_brewer(palette="Dark2") +
  xlab("Std amplitude") + ylab("Average amplitude") +
  theme(aspect.ratio = 1)
```

Abba is just different from everyone else!


## Education

### Data description

- OECD PISA survey is the world's global metric for quality, equity and efficiency in school education.
- Workforce readiness of 15-year old students
- 14530 students tested in Australia in 2015
- How many schools?
- Math, reading and science tests, surveys on school and home environment, 921 variables
- Data available from [http://www.oecd.org/pisa/data/](http://www.oecd.org/pisa/data/)

```{r eval=FALSE}
pisa_2015 <- read_sav(file.choose()) ## The SPSS format zip file
pisa_au <- pisa_2015 %>% filter(CNT == "AUS")
save(pisa_au, file="data/pisa_au.rda")
```


### Gender differences

```{r}
load("data/pisa_au.rda")
pisa_au <- pisa_au %>% 
  mutate(ST004D01T = factor(ST004D01T, 
                            levels=c(1,2), 
                            labels=c("girls", "boys"))) 
pisa_au %>%
  group_by(ST004D01T) %>%
  summarise(math=weighted.mean(PV1MATH, SENWT), 
            read=weighted.mean(PV1READ, SENWT), 
            sci=weighted.mean(PV1SCIE, SENWT))
p1 <- ggplot(pisa_au, aes(x=ST004D01T, y=PV1MATH, 
                          fill=ST004D01T)) +
  geom_boxplot() + xlab("") + theme(legend.position="none")
p2 <- ggplot(pisa_au, aes(x=ST004D01T, y=PV1READ, 
                          fill=ST004D01T)) +
  geom_boxplot() + xlab("") + theme(legend.position="none")
p3 <- ggplot(pisa_au, aes(x=ST004D01T, y=PV1SCIE, 
                          fill=ST004D01T)) +
  geom_boxplot() + xlab("") + theme(legend.position="none")
grid.arrange(p1, p2, p3, ncol=3)
```

On average reading scores are most different, with girls scoring substantially higher. There is more variability from individual to individual than from boys to girls. 


## Tennis

Statistics for grand slam and top tournament matches are available through the ATP and WTA web sites. By web scraping these web sites we can pull data together to explore characteristics of different players, surfaces, ... These are compiled into the R package deuce. 

```{r}
## devtools::install_github("skoval/deuce")
library(deuce)
data(wta_matches)
grand_slams <- filter(wta_matches, 
                      tourney_level == "Grand Slams", 
                      year>1983)
p <- ggplot(grand_slams, aes(x=factor(year), y=winner_age)) + 
  geom_boxplot() + 
  scale_x_discrete(breaks=c("1990", "2000", "2010", "2020")) + 
  xlab("") + ylab("winner age") 
p <- p +
  geom_point(mapping=aes(text=winner_name), 
             data=filter(grand_slams, winner_age > 35), 
             alpha=0.5, colour="red", size=3) 
ggplotly(p)
```

Yes, there are 47 year olds playing Grand Slams! [See NYTimes, 2004](http://www.nytimes.com/2004/06/21/sports/tennis/navratilova-47-wins-at-wimbledon.html)

## Australian Politics

### AEC data

The [Australian Electoral Commission](http://www.aec.gov.au/elections/federal_elections/2016/downloads.htm) provides Federal election results, and the electoral map. 

```{r fig.width=5, fig.height=4}
library(eechidna)
who_won <- aec2016_2pp_electorate %>% 
  group_by(PartyNm) %>% 
  tally() %>% 
  arrange(desc(n)) 

## plot
who_won <- aec2016_2pp_electorate %>% 
  group_by(PartyNm) %>% 
  tally() %>% 
  arrange(desc(n)) 

## plot
ggplot(who_won, 
       aes(reorder(PartyNm, n), 
           n)) +
  geom_point(size = 3) + 
  coord_flip() + 
  theme_bw() +
  ylab("Total number of electorates") +
  xlab("") +
  theme(text = element_text(size=10))
```

### Map it!

```{r fig.width=6, fig.height=4}
data(nat_data_2016_cart)
data(nat_map_2016)
data(aec2016_fp_electorate)
map.winners <- aec2016_fp_electorate %>% filter(Elected == "Y") %>% 
  select(Electorate, PartyNm) %>% 
  merge(nat_map_2016, by.x="Electorate", by.y="Elect_div")

## Grouping different Lib/Nats togethers
map.winners$PartyNm <- as.character(map.winners$PartyNm)
coalition <- c("Country Liberals (NT)", "Liberal", 
               "Liberal National Party of Queensland", "The Nationals")
map.winners.grouped <- mutate(map.winners, 
    PartyNm = ifelse(as.character(PartyNm) %in% coalition,
       "Liberal National Coalition", PartyNm))
map.winners.grouped <- map.winners.grouped %>% arrange(group, order)

## Colour cells to match that parties colours
## Order = Australian Labor Party, Independent, Katters, Lib/Nats Coalition, Palmer, The Greens
partycolours = c("##FF0033", "##000000", "##CC3300", "##0066CC", "##FFFF00", "##009900")

ggplot(data=map.winners.grouped) + 
  geom_polygon(aes(x=long, y=lat, group=group,
                   fill=PartyNm)) +
  #scale_fill_manual(name="Political Party",
  #                  values=partycolours) +
  theme_map() + coord_equal() + theme(legend.position="bottom")
```

### Cartogram it!

```{r fig.width=7, fig.height=5}
## Load election results
cart.winners <- aec2016_fp_electorate %>% filter(Elected == "Y") %>% 
  select(Electorate, PartyNm) %>% 
  merge(nat_data_2016_cart, by.x="Electorate", by.y="ELECT_DIV")

## Grouping different Lib/Nats togethers
cart.winners$PartyNm <- as.character(cart.winners$PartyNm)
coalition <- c("Country Liberals (NT)", "Liberal", "Liberal National Party of Queensland",
               "The Nationals")
cart.winners.grouped <- mutate(cart.winners, 
  PartyNm = ifelse(as.character(PartyNm) %in% coalition, 
                   "Liberal National Coalition", PartyNm))

## Plot it
ggplot(data=nat_map_2016) +
  geom_polygon(aes(x=long, y=lat, group=group, order=order),
               fill="grey90", colour="white") +
  geom_point(data=cart.winners.grouped, aes(x=x, y=y, colour=PartyNm), size=2, alpha=0.8) +
#  scale_colour_manual(name="Political Party", values=partycolours) +
  theme_map() + coord_equal() + theme(legend.position="bottom")
```

### Combine with Census data

```{r}
## devtools::install_github("ijlyttle/vembedr")
library(vembedr)
embed_vimeo("167367369", width=800, height=500)
```

Video link: [https://vimeo.com/167367369](https://vimeo.com/167367369)

## About Melbourne

### Pedestrian sensors

```{r fig.width=10, fig.height=3}
ped <- read_csv("http://www.pedestrian.melbourne.vic.gov.au/datadownload/January_2018.csv")
ped_m <- ped %>% 
  mutate(Date = dmy(Date)) %>%
  select(Date, Hour, `Melbourne Central`, `Flinders St Station Underpass`, `Birrarung Marr`) %>%
  gather(location, count, `Melbourne Central`, `Flinders St Station Underpass`, `Birrarung Marr`) %>%
    mutate(wday=wday(Date, label=TRUE), mday=mday(Date),
           wk=week(Date)) ##%>%
    ##mutate(wday=recode_factor(wday,
    ##    levels=c("Mon","Tues","Wed","Thurs","Fri","Sat","Sun")))
ggplot(ped_m, aes(x=Hour, y=count, 
                       colour=location, 
                       group=interaction(Date, location))) +
  scale_colour_brewer(palette="Dark2") +
  facet_wrap(~wday, ncol=7) +
  geom_line() + theme(legend.position="bottom")
```

### Each day 

```{r fig.height=7}
ggplot(ped_m, aes(x=Hour, y=count, 
                       colour=location)) +
  scale_colour_brewer(palette="Dark2") +
  facet_grid(wk~wday) +
  geom_line() + theme(legend.position="bottom")
```

### Google maps

Pull a google map and see where the sensors are located.

```{r}
## devtools::install_github("dkahle/ggmap")
locations <- read_csv("data/Pedestrian_sensor_locations.csv")
library(ggmap)
ctr <- locations %>% 
  summarise(lat=mean(Latitude), lon=mean(Longitude))
map <- get_map(c(ctr$lon, ctr$lat), zoom=14)
ggmap(map) + geom_point(data=locations, 
                        mapping=aes(x=Longitude, y=Latitude)) +
  theme_map()
```

## Airline traffic

### Data source

The web site [https://www.transtats.bts.gov](https://www.transtats.bts.gov) keeps records for all commercial flights in the USA. The ontime database is interesting. You can download all records since record collection started, about 20Gb, or select months. I've pulled records for May 2017, 20Mb zip'd data.

### Variables

```{r eval=FALSE, echo=TRUE}
"YEAR","MONTH","DAY_OF_MONTH","DAY_OF_WEEK",
"FL_DATE","UNIQUE_CARRIER","AIRLINE_ID","CARRIER",
"TAIL_NUM","FL_NUM","ORIGIN_AIRPORT_ID",
"ORIGIN_AIRPORT_SEQ_ID","ORIGIN_CITY_MARKET_ID",
"DEST_AIRPORT_ID","DEST_AIRPORT_SEQ_ID",
"DEST_CITY_MARKET_ID","CRS_DEP_TIME","DEP_TIME",
"DEP_DELAY","DEP_DELAY_NEW","DEP_DEL15",
"DEP_DELAY_GROUP","TAXI_OUT","WHEELS_OFF","WHEELS_ON",
"TAXI_IN","CRS_ARR_TIME","ARR_TIME","ARR_DELAY",
"ARR_DELAY_NEW","ARR_DEL15","CANCELLED",
"CANCELLATION_CODE","DIVERTED","CRS_ELAPSED_TIME",
"ACTUAL_ELAPSED_TIME","AIR_TIME","DISTANCE",
"CARRIER_DELAY","WEATHER_DELAY","NAS_DELAY",
"SECURITY_DELAY","LATE_AIRCRAFT_DELAY"
```

### Where did my plane fly?

```{r eval=FALSE}
air <- read_csv("data/901848483_T_ONTIME.csv")

dfw_dsm <- air %>%
  filter(CARRIER == "AA", FL_DATE == "2017-05-06",
         ORIGIN == "DFW", DEST == "DSM")
dfw_dsm %>%
  select(CRS_DEP_TIME, TAIL_NUM)

plane_N4YRAA <- air %>% filter(TAIL_NUM == "N4YRAA") %>%
  select(FL_DATE, CARRIER, FL_NUM, ORIGIN, DEST, DEP_TIME, ARR_TIME, DISTANCE)
save(plane_N4YRAA, file="data/plane_N4YRAA.rda")
```

```{r fig.width=6, fig.height=6}
load("data/plane_N4YRAA.rda")
airport <- read_csv("data/airports.csv")
airport <- airport %>%
  select(AIRPORT, LATITUDE, LONGITUDE, AIRPORT_IS_LATEST, DISPLAY_AIRPORT_NAME) %>%
  filter(AIRPORT_IS_LATEST == 1) %>%
  select(-AIRPORT_IS_LATEST)

N4YRAA_latlon <- left_join(plane_N4YRAA, airport,
                           by = c("ORIGIN"="AIRPORT")) %>%
  rename("ORIGIN_LATITUDE"="LATITUDE",
         "ORIGIN_LONGITUDE"="LONGITUDE")

N4YRAA_latlon <- left_join(N4YRAA_latlon, airport,
                           by = c("DEST"="AIRPORT")) %>%
  rename("DEST_LATITUDE"="LATITUDE",
         "DEST_LONGITUDE"="LONGITUDE")

N4YRAA_latlon <- N4YRAA_latlon %>% arrange(FL_DATE, DEP_TIME)

map <- get_map(c(lon=-92.20562, lat=36.20259), zoom=5)
ggmap(map) +
  geom_segment(data=filter(N4YRAA_latlon,
                           FL_DATE == ymd("2017-05-06")),
               aes(x=ORIGIN_LONGITUDE, xend=DEST_LONGITUDE,
                   y=ORIGIN_LATITUDE, yend=DEST_LATITUDE),
               color="navyblue") +
  geom_point(data=filter(N4YRAA_latlon,
                         FL_DATE == ymd("2017-05-06")),
             aes(x=ORIGIN_LONGITUDE,
                 y=ORIGIN_LATITUDE), color="orange",
             alpha=0.3, size=3) +
  geom_point(data=filter(N4YRAA_latlon,
                         FL_DATE == ymd("2017-05-06")),
             aes(x=DEST_LONGITUDE,
                 y=DEST_LATITUDE), color="red",
             alpha=0.3, size=1) +
   theme_map()
```

### and in the rest of the month ...

```{r fig.width=10, fig.height=6}
map <- get_map(c(lon=-92.20562, lat=36.20259), zoom=4)
ggmap(map) +
  geom_segment(data=filter(N4YRAA_latlon,
                           FL_DATE < ymd("2017-05-15")),
               aes(x=ORIGIN_LONGITUDE, xend=DEST_LONGITUDE,
                   y=ORIGIN_LATITUDE, yend=DEST_LATITUDE),
               color="navyblue") +
  geom_point(data=filter(N4YRAA_latlon,
                         FL_DATE < ymd("2017-05-15")),
             aes(x=ORIGIN_LONGITUDE,
                 y=ORIGIN_LATITUDE), color="orange",
             alpha=0.3, size=3) +
  geom_point(data=filter(N4YRAA_latlon,
                         FL_DATE < ymd("2017-05-15")),
             aes(x=DEST_LONGITUDE,
                 y=DEST_LATITUDE), color="red",
             alpha=0.3, size=1) +
  facet_wrap(~FL_DATE, ncol=7) + 
  theme_map()
```

### What airports are these

```{r}
library(knitr)
origin_airports <- N4YRAA_latlon %>% 
  select(ORIGIN) %>%
  distinct() 
airport %>% filter(AIRPORT %in% origin_airports$ORIGIN) %>%
  select(AIRPORT, DISPLAY_AIRPORT_NAME) %>% kable()
```

## Financial data 

### Cross-rates relative to USD

Pull historical exchange rates from [http://openexchangerates.org/api/historical/](http://openexchangerates.org/api/historical/). 

```{r eval=FALSE}
library(jsonlite)
ru <- NULL
dt <- as.Date("2017-06-20")
for (i in 3:27) {
  cat(i,"\n")
  url <- paste("http://openexchangerates.org/api/historical/",dt,".json?app_id=XXXX", sep="")
  x <- fromJSON(url)
  x <- x$rates
  if (length(x) == 171) 
    x <- x[-c(164,166)]
  ru <- rbind(ru, data.frame(date=dt, x))
  dt <- dt + days(1)
}
rownames(ru) <- ru$date
write_csv(ru, path="data/rates.csv")
```

```{r}
rates <- read_csv("data/rates.csv")
rates_m <- rates %>% select(date, AUD, NZD, GBP) %>%
  mutate_at(vars(AUD, NZD, GBP), scale) %>%
  gather(currency, rate, -date)
p1 <- ggplot(rates_m, aes(x=date, y=rate, colour=currency)) + 
  geom_line() + xlab("Standardised rates") +
  scale_colour_brewer(palette="Dark2") 
p2 <- ggplot(rates, aes(AUD, NZD)) + 
  geom_point() + ##geom_line() +
  theme(aspect.ratio=1)
p3 <- ggplot(rates, aes(AUD, GBP)) + 
  geom_point() + ##geom_line() +
  theme(aspect.ratio=1)
p4 <- ggplot(rates, aes(NZD, GBP)) + 
  geom_point() + ##geom_line() +
  theme(aspect.ratio=1)
library(ggplot2)
library(gridExtra)
grid.arrange(p1, p2, p3, p4, 
             layout_matrix = matrix(c(1, 1, 1, 2, 3, 4), 
                                    ncol=3, byrow=TRUE))
```

## Text analysis

### Analysing tweets

Is it possible to distinguish tweets coming from Donald Trump's phone vs his staff's phone? With a twitter api you can collect all tweets between certain times, from different people, with different hashtags, ... David Robinson wrote a [post](http://varianceexplained.org/r/trump-tweets/) during last year's US election cycle doing just this. Here's a re-creation of his analysis.

Tweets from @realDonaldTrump were collected and passed through a sentiment analysis.

```{r}
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
tweets <- trump_tweets_df %>%
  select(id, statusSource, text, created) %>%
  extract(statusSource, "source", "Twitter for (.*?)<") %>%
  filter(source %in% c("iPhone", "Android"))
tweets %>%
  count(source, hour = hour(with_tz(created, "EST"))) %>%
  mutate(percent = n / sum(n)*100) %>%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "")
```

### Common words between devices

```{r fig.height=7}
library(tidytext)
library(stringr)

reg <- "([^A-Za-z\\d##@']|'(?![A-Za-z\\d##@]))"
tweet_words <- tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
android_iphone_ratios <- tweet_words %>%
  count(word, source) %>%
  filter(sum(n) >= 5) %>%
  spread(source, n, fill = 0) %>%
  mutate_at(vars(Android, iPhone), 
            funs((. + 1) / sum(. + 1))) %>%
  mutate(logratio = log2(Android / iPhone)) %>%
  arrange(desc(logratio))
android_iphone_ratios %>%
  group_by(logratio > 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Android / iPhone log ratio") +
  scale_fill_manual(name = "", labels = c("Android", "iPhone"),
                    values = c("red", "lightblue"))
```

### Sentiment analysis

Poisson test of the differences between whether it is mode likely to emerge from the Android.

```{r results='hide'}
nrc <- sentiments %>%
  filter(lexicon == "nrc") %>%
  dplyr::select(word, sentiment)

sources <- tweet_words %>%
  group_by(source) %>%
  mutate(total_words = n()) %>%
  ungroup() %>%
  distinct(id, source, total_words)

by_source_sentiment <- tweet_words %>%
  inner_join(nrc, by = "word") %>%
  count(sentiment, id) %>%
  ungroup() %>%
  complete(sentiment, id, fill = list(n = 0)) %>%
  inner_join(sources) %>%
  group_by(source, sentiment, total_words) %>%
  summarize(words = sum(n)) %>%
  ungroup()

library(broom)
sentiment_differences <- by_source_sentiment %>%
  group_by(sentiment) %>%
  do(tidy(poisson.test(.$words, .$total_words))) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, estimate))
```

```{r fig.width=6, fig.height=5}
ggplot(sentiment_differences, aes(x=sentiment, y=estimate)) + 
  geom_hline(yintercept=1, colour="white", size=3) +
  geom_point() + 
  geom_errorbar(aes(x=sentiment, ymin=conf.low, ymax=conf.high)) +
  scale_y_continuous(breaks=seq(0.7, 2.2, 0.1)) + 
  coord_flip()
```

## Climate change

### CO2 monitoring

- Data is collected at a number of locations world wide. 
- See [Scripps Inst. of Oceanography](http://scrippsco2.ucsd.edu/data/atmospheric_co2) 
- Let's pull the data from the web and take a look ...
- 
- Recordings from South Pole (SPO), Kermadec Islands (KER), Mauna Loa Hawaii (MLF), La Jolla Pier, California (LJO), Point Barrow, Alaska (PTB).

### Read data and plot

```{r CO2, fig.width=10, fig.height=5}
CO2.ptb<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_ptb.csv", sep=",", skip=69)
colnames(CO2.ptb)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.ptb$lat<-71.3
CO2.ptb$lon<-(-156.6)
CO2.ptb$stn<-"ptb"

CO2.ljo<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_ljo.csv", sep=",", skip=69)
colnames(CO2.ljo)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.ljo$lat<-32.9
CO2.ljo$lon<-(-117.3)
CO2.ljo$stn<-"ljo"

CO2.spo<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_spo.csv", sep=",", skip=69)
colnames(CO2.spo)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.spo$lat<- (-90.0)
CO2.spo$lon<-0
CO2.spo$stn<-"spo"

CO2.ker<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_ker.csv", sep=",", skip=69)
colnames(CO2.ker)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.ker$lat<-(-29.2)
CO2.ker$lon<-(-177.9)
CO2.ker$stn<-"ker"

CO2.all<-rbind(CO2.ker,CO2.ljo,CO2.ptb,CO2.spo)
CO2.all$date<-as.Date(CO2.all$date)

CO2.all$invlat=-1*CO2.all$lat
CO2.all$stn=reorder(CO2.all$stn,CO2.all$invlat)

CO2.all.loc <- rbind(CO2.ker[1,],CO2.ljo[1,],CO2.ptb[1,],CO2.spo[1,])

p1 <- qplot(date, co2, data=subset(CO2.all, flg < 2), colour=stn, geom="line",xlab="Year",ylab="CO2 (ppm)") + 
		facet_wrap(~stn, ncol=1) + theme(axis.text.y=element_text(size = 6), legend.position="none")
p2 <- qplot(date, co2, data=subset(CO2.all, flg < 2), colour=stn, geom="line",xlab="Year",ylab="CO2 (ppm)") + 
  theme(axis.text.y=element_text(size = 6), legend.position="none")
grid.arrange(p1, p2, ncol=2)
```

### on a map

```{r CO2-map, fig.width=4.5, fig.height=2.5}
world <- map_data("world")
worldmap <- ggplot(world, aes(x=long, y=lat, group=group)) +
  geom_path(color="grey80", size=0.5) + xlab("") + ylab("") +
  scale_y_continuous(breaks=(-2:2) * 30) +
  scale_x_continuous(breaks=(-4:4) * 45) +
  theme_bw() + theme(aspect.ratio=0.6)
worldmap + geom_point(data=CO2.all.loc, aes(x=lon, y=lat, group=1), colour="red", 
                      size=2, alpha=0) +
  geom_text(data=CO2.all.loc, aes(x=lon, y=lat, label=stn, group=1), 
            colour="orange", size=5)
```

- CO2 is increasing, and it looks like it is exponential increase. 
- The same trend is seen at every location - REALLY? Need some physics to understand this.
- Some stations show seasonal pattern - actually the more north the more seasonality - WHY?

## What is R?

### R is ...

* Most commonly used data science software [kdnuggets](http://www.kdnuggets.com/2017/05/poll-analytics-data-science-machine-learning-software-leaders.html)
* __Free__ to use, __open source__ so you can see what code is doing to your data
* __Extensible__: Over 11000 user contributed add-on packages currently on CRAN! Bioconductor has more than 1300 packages, and many researchers provide packages through github.
* __Powerful__: With the right tools, get more work done, faster, better.
* __Flexible__: Not a question of _can_, but _how_.

```{r, eval = FALSE, echo = FALSE}
## devtools::install_github("metacran/crandb")
## pkgs <- crandb::list_packages(limit = 999999)
## length(pkgs)
## [1] 12194
```

### RStudio is ...

[From Julie Lowndes](http://jules32.github.io/resources/RStudio_intro/):  

<blockquote>
<b>If R were an airplane, RStudio would be the airport</b>, providing many, many supporting services that make it easier for you, the pilot, to take off and go to awesome places. Sure, you can fly an airplane without an airport, but having those runways and supporting infrastructure is a game-changer.
</blockquote>

### The RStudio IDE

- Source editor: Docking station for multiple files, Useful shortcuts ("Knit"), Highlighting/Tab-completion, Code-checking (R, HTML, JS), Debugging features  
- Console window: Highlighting/Tab-completion, Search recent commands
- Other tabs/panes: Graphics, R documentation, Environment pane, File system navigation/access, Tools for package development, git, etc

### Installing packages

From CRAN

```{r eval=FALSE, echo=TRUE}
install.packages("learnr")
```


From bioconductor

```{r eval=FALSE, echo=TRUE}
source("https://bioconductor.org/biocLite.R")
biocLite("ggbio")
```

From github repos

```{r eval=FALSE, echo=TRUE}
devtools::install_github("earowang/sugrrants")
devtools::install_github("haleyjeppson/ggmosaic")
```

### What is R Markdown?

From the [R Markdown home page](http://rmarkdown.rstudio.com/):
- R Markdown is an authoring format that enables easy creation of dynamic documents, presentations, and reports from R. 
- It combines the core syntax of __markdown__ (an easy-to-write plain text format) __with embedded R code chunks__ that are run so their output can be included in the final document. 
- R Markdown documents are fully reproducible (they can be automatically regenerated whenever underlying R code or data changes).

### Open data, open source

- Data is available everywhere today, publicly, free
- Software, very powerful software, for analysis of data is available publicly, free
- Combined with a knowledge of mathematics and statistics empowers each of us to contribute to understand and improve our world

## Share and share alike

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.

