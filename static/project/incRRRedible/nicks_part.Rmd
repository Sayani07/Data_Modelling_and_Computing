---
title: "ETC1010: Data Modelling and Computing"
author: "Professor Di Cook, EBS, Monash U."
output: 
  learnr::tutorial:
    css: "css/logo.css"
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)

knitr::opts_chunk$set(echo = TRUE,   
                      message = FALSE,
                      warning = FALSE,
                      collapse = TRUE,
                      fig.height = 4,
                      fig.width = 8,
                      fig.align = "center",
                      cache = FALSE,
                      comment = NA )
tutorial_html_dependency()
version
```



```{r include=FALSE}
library(learnr)
library(tidyverse)
library(dplyr)
library(lubridate)
library(deuce)
library(plyr)
library(DescTools)
library(ggplot2)
data("atp_elo")

data("atp_rankings")
data("atp_players")

data("atp_elo")
head(atp_elo)
str(atp_elo)

data("atp_rankings")
head(atp_rankings)
str(atp_rankings)

data("atp_players")
head(atp_players)
str(atp_players)

data("atp_matches")
head(atp_matches)
str(atp_matches)

data("atp_tournaments")
head(atp_tournaments)
str(atp_tournaments)
```

## Predicting players ATP rankings in ten years

In this section, we will attempt to discover if the ranking of an ATP player can be predicted to any degree of reliability through looking at their ranking in the early days of their proffesional career and their overall performance in ATP tournaments with a special focus on grand slams. This section continues to use the deuce package, and the first thing to do here is to obtain a profile of each player, with their rankings and some other personal information, such as date of birth. 
This is accomplished through the atp_rankings dataset, which is joined to the atp_players table, a second table which contains a more fleshed out version of the players name, date of birth. The result of this join is displayed below: 

```{r include=FALSE}
#We can use fetch_activity function to find out when these players first appeared
#I need to somehow match up player ID and player name
#The atp_players table matches this - let's join the player names, IDs and rankings - here goes

rankings <- join(atp_rankings, atp_players, by = "player_id", type = "inner")


str(rankings)
```
```{r echo=FALSE}
head(rankings)
```


We would now like to split the rankings of players into their ATP ranking points after the first 3,6,9 months, then 1 through to 5 years of their arrival on the ATP tour. This is done by taking the ranking table formed previously, grouping by player, and filtering out all dates outside the time period of interest (so to determine ATP ranking points after 6 months, we select the last recorded value of ATP ranking points after filtering out dates beyond 6 months after their first recorded date). This results in tables for each of the datasets, which are then joined together to create a single time breakdown of the progression of these players ATP ranking points. 


```{r include=FALSE}
#now I have ranking points and players, I now need to know how to plot the amount of ranking points in the first 3 months, 6 months, 9 months and 12 months - how can I do that?

three_months <- rankings %>% arrange(date) %>% group_by(name) %>% arrange(date) %>% filter(date <= AddMonths(date[1], 3)) %>% filter(date == date[length(date)]) %>% select(c(name, date, ranking_points)) %>% rename(c("ranking_points" = "rnkng_pnts_3mnths", "date" = "date_3mnths"))


six_months <- rankings %>% arrange(date) %>% group_by(name) %>% arrange(date) %>% filter(date <= AddMonths(date[1], 6)) %>% filter(date == date[length(date)]) %>% select(c(name, date, ranking_points)) %>% rename(c("ranking_points" = "rnkng_pnts_6mnths", "date" = "date_6mnths"))


nine_months <- rankings %>% arrange(date) %>% group_by(name) %>% arrange(date) %>% filter(date <= AddMonths(date[1], 9)) %>% filter(date == date[length(date)]) %>% select(c(name, date, ranking_points)) %>% rename(c("ranking_points" = "rnkng_pnts_9mnths", "date" = "date_9mnths"))


one_year <- rankings %>% arrange(date) %>% group_by(name) %>% arrange(date) %>% filter(date <= AddMonths(date[1], 12)) %>% filter(date == date[length(date)]) %>% select(c(name, date, ranking_points)) %>% rename(c("ranking_points" = "rnkng_pnts_1yr", "date" = "date_1yr"))


two_year <- rankings %>% arrange(date) %>% group_by(name) %>% arrange(date) %>% filter(date <= AddMonths(date[1], 24)) %>% filter(date == date[length(date)]) %>% select(c(name, date, ranking_points)) %>% rename(c("ranking_points" = "rnkng_pnts_2yr", "date" = "date_2yr"))


three_year <- rankings %>% arrange(date) %>% group_by(name) %>% arrange(date) %>% filter(date <= AddMonths(date[1], 36)) %>% filter(date == date[length(date)]) %>% select(c(name, date, ranking_points)) %>% rename(c("ranking_points" = "rnkng_pnts_3yr", "date" = "date_3yr"))


four_year <- rankings %>% arrange(date) %>% group_by(name) %>% arrange(date) %>% filter(date <= AddMonths(date[1], 48)) %>% filter(date == date[length(date)]) %>% select(c(name, date, ranking_points)) %>% rename(c("ranking_points" = "rnkng_pnts_4yr", "date" = "date_4yr"))


five_year <- rankings %>% arrange(date) %>% group_by(name) %>% arrange(date) %>% filter(date <= AddMonths(date[1], 60)) %>% filter(date == date[length(date)]) %>% select(c(name, date, ranking_points)) %>% rename(c("ranking_points" = "rnkng_pnts_5yr", "date" = "date_5yr"))


ten_year <- rankings %>% arrange(date) %>% group_by(name) %>% arrange(date) %>% filter(date <= AddMonths(date[1], 120)) %>% filter(date == date[length(date)]) %>% select(c(name, date, ranking_points)) %>% rename(c("ranking_points" = "rnkng_pnts_10yr", "date" = "date_10yr"))

```


```{r include=FALSE}
three_six <- merge(three_months, six_months, by = "name")
three_six_nine <- merge(three_six, nine_months, by = "name")
three_six_nine_1yr <- merge(three_six_nine, one_year, by = "name")
three_six_nine_1yr_2yr <- merge(three_six_nine_1yr,two_year, by = "name")
three_six_nine_1yr_2yr_3yr <- merge(three_six_nine_1yr_2yr,three_year, by = "name")
three_six_nine_1yr_2yr_3yr_4yr <- merge(three_six_nine_1yr_2yr_3yr,four_year, by = "name")
three_six_nine_1yr_2yr_3yr_4yr_5yr <- merge(three_six_nine_1yr_2yr_3yr_4yr,five_year, by = "name")

player_stats <- merge(three_six_nine_1yr_2yr_3yr_4yr_5yr, ten_year, by = "name")

head(player_stats)
str(player_stats)

## OK we can see there are 13 variables in this data frame of which 6 are meaningful for PCA 
## dates variables are not meaningful here.
```

### OBTAINING SOME INDICATION GRAND SLAM PERFORMANCE

How players perform in the glamorous and exciting grand slams over the years may well prove to be a predictor of ranking points after a solid amount of time competing at the top level (10 years, hopefully). Below is calculated the number of wins and losses recorded for each player in grand slams. It is not however, a good idea to split this variable into similar time frames as the ranking variables above, as some players may not have even played a grand slam in their first year, or even 2 or 3 years on tour. Therefore, we take this observation after 4 years.

This is found by taking the table provided by the deuce package, atp_matches (containing point by point breakdown statistics of many grand slam matches over the course of history), and joining this dataset with that describing ATP ranking points after 4 years. The date in this second table provides the cutoff date past which we should not consider grand slam results. A snippet of the code to produce this overall dataset is shown below: 

Separate tables are created for wins and losses in grand slams, and finally they are joined to create the final dataset.

```{r echo=TRUE}
#******************************************4 YEAR GS************************************************************
atp_matches <- rename(atp_matches,replace = c("winner_name" = "name"))

tournaments_4yr <- join(atp_matches, four_year, by = "name") %>% select(c(tourney_name, tourney_level, name, winner_rank, winner_rank_points, loser_name, loser_rank, loser_rank_points, tourney_start_date, date_4yr, rnkng_pnts_4yr))


gs_4yr <- tournaments_4yr %>% filter(tourney_level == "Grand Slams") %>% group_by(name) %>% filter(tourney_start_date <= date_4yr)
gs_4yr_wins <- count(gs_4yr$name) %>% rename(c("x" = "name", "freq" = "gs_wins"))
gs_4yr_wins <- join(gs_4yr_wins, four_year, by = "name") %>% separate(date_4yr, into = c("year_4yr", "month_4yr", "day_4yr")) %>% select(name, gs_wins)


gs_4yr <- tournaments_4yr %>% filter(tourney_level == "Grand Slams") %>% group_by(loser_name) %>% filter(tourney_start_date <= date_4yr)
gs_4yr_losses <- count(gs_4yr$loser_name) %>% rename(c("x" = "name", "freq" = "gs_losses"))
gs_4yr_losses <- join(gs_4yr_losses, four_year, by = "name") %>% separate(date_4yr, into = c("year_4yr", "month_4yr", "day_4yr")) %>% select(name, gs_losses)


gs_4yr_wlr <- join(gs_4yr_losses, gs_4yr_wins, by = "name")

head(gs_4yr_wlr)


```



### OBTAINING OVERALL ATP TOURNAMENT PERFORMANCE

In tennis, there is a fair number of players who may do well in tournaments other than grand slams, but who repeatedly fail to bring it together on the big stage. (Alexander Zverev and Nick Kyrgios are debatedly two of those, just to name a few.) We thought it would be good to include some reference to these other tournaments as indicators of performance on the ATP tour. 

Luckily this information was available in the atp_matches table once more, and allowed us to focus on all professional level tournaments including grand slams. This table was again joined to the four year ranking table, or the same reason as it was in the previous step (obtainig grand slam performance). Four years was chosen as the cutoff again, to make the wins and losses obtained here comparable to those of the grand slam data.

```{r echo=FALSE}
#*******************************************ALL ATP TOUNAMENTS 4 YEAR****************************************
tournaments_4yr <- join(atp_matches, four_year, by = "name") %>% select(c(tourney_name, tourney_level, name, winner_rank, winner_rank_points, loser_name, loser_rank, loser_rank_points, tourney_start_date, date_4yr, rnkng_pnts_4yr))

ATP_4yr <- tournaments_4yr %>% filter(tourney_level == "Grand Slams" | tourney_level == "250 or 500" | tourney_level == "Davis Cup" | tourney_level == "Tour Finals" | tourney_level == "Masters") %>% group_by(name) %>% filter(tourney_start_date <= date_4yr)
ATP_4yr_wins <- count(ATP_4yr$name)%>% rename(c("x" = "name", "freq" = "atp_wins"))
ATP_4yr_wins <- join(ATP_4yr_wins, four_year, by = "name") %>% separate(date_4yr, into = c("year_4yr", "month_4yr", "day_4yr"))

ATP_4yr <- tournaments_4yr %>% filter(tourney_level == "Grand Slams" | tourney_level == "250 or 500" | tourney_level == "Davis Cup" | tourney_level == "Tour Finals" | tourney_level == "Masters") %>% group_by(loser_name) %>% filter(tourney_start_date <= date_4yr)
ATP_4yr_losses <- count(ATP_4yr$loser_name)%>% rename(c("x" = "name", "freq" = "atp_losses"))


ATP_4yr_wlr <- join(ATP_4yr_wins, ATP_4yr_losses, by = "name")


atp <- ATP_4yr_wlr %>% select(name, atp_wins, atp_losses)


```





```{r echo=FALSE}
## so lets select out the relevant variables we want and call this ranking_variables

library(dplyr)

ranking_variables <- player_stats %>%
  select(name, rnkng_pnts_3mnths, rnkng_pnts_6mnths, rnkng_pnts_9mnths, 
              rnkng_pnts_1yr, rnkng_pnts_2yr, rnkng_pnts_3yr, rnkng_pnts_4yr, rnkng_pnts_5yr, rnkng_pnts_10yr)

str(ranking_variables)
head(ranking_variables)
```


Having obtained this data we could now join the atp tournaments, grand slams and ranking statistics and combine them all in one table, shown below. 

```{r echo=TRUE}
##join atp tournament performance to ranking data
ranking_variables_distinct <- merge(ranking_variables, atp, by = "name")


##have to do this joining BEFORE ADJUSTING FOR DUPLICATES
ranking_variables_distinct <- merge(ranking_variables_distinct, gs_4yr_wlr, by = "name")


head(ranking_variables_distinct)
```

The dataset contained some duplicate rows as some players had identical names! This was dealt with by adjusting the players name each time it came up, adding an incrementing number to the end of it e.g. William Brown 1, William Brown 2 etc, if there were multiple instances of William Brown for some reason.

```{r echo=FALSE}
## we need to remove duplicate names

## distinct() [dplyr package] to remove duplicate rows in a data frame.

ranking_variables_distinct <- distinct(ranking_variables_distinct)


str(ranking_variables_distinct)
```


```{r echo=FALSE}

##of players with identical names, keep only one
##**************************************DON'T HAVE TO DO THIS***********************************************************************
ranking_variables_distinct$name[581:584] <- c("Alexander Zlatnik 1", "Alexander Zlatnik 2", "Alexander Zlatnik 3", "Alexander Zlatnik 4" )


ranking_variables_distinct$name[773:774] <- c("Amir Reza 1", "Amir Reza 2")


ranking_variables_distinct$name[890:897] <- c("Andrea Turini 1", "Andrea Turini 2", "Andrea Turini 3", "Andrea Turini 4", "Andrea Turini 5", "Andrea Turini 6", "Andrea Turini 7", "Andrea Turini 8" )


#ranking_variables_distinct$name[2760:2761] <- c("Dane Mcgregor 1", "Dane Mcgregor 2")


##Federico Iannaccone has too many
##so remove the guy and continue

ranking_variables_distinct <- ranking_variables_distinct %>% filter(name != c("Federico Iannaccone"))


#ranking_variables_distinct$name[7636:7637] <- c("Kenichi Kiyomiya 1", "Kenichi Kiyomiya 2")


#ranking_variables_distinct$name[7380:7381] <- c("Kenichi Kiyomiya 3", "Kenichi Kiyomiya 4")


#ranking_variables_distinct$name[9161:9162] <- c("Mathias Hellstrom 1", "Mathias Hellstrom 2")


#ranking_variables_distinct$name[8905:8906] <- c("Mathias Hellstrom 3", "Mathias Hellstrom 4")

##***********************************************************************************************************

##*************************************************************************************************
##remove duplicates HERE!!
ranking_variables_distinct$name[917:922] <- c("Saeed Meer 1", "Saeed Meer 2", "Saeed Meer 3", "Saeed Meer 4", "Saeed Meer 5", "Saeed Meer 6" )


ranking_variables_distinct$name[1077:1084] <- c("William Brown 1", "William Brown 2", "William Brown 3", "William Brown 4", "William Brown 5", "William Brown 6", "William Brown 7", "William Brown 8")


ranking_variables_distinct$name[1050:1053] <- c("Terry Ryan 1", "Terry Ryan 2", "Terry Ryan 3", "Terry Ryan 4" )

##***********************************************************************************************

```

### PCA ANALYSIS ON THE VARIABLES ON THIS DATASET
In this section we conduct principal component analysis on all the variables in this dataset. This will allow us to identify which variables are intercorrelated, and which of them are most important in explaining variation among them. The variables will be separated into principal components which will highlight the most important information in the dataset. 

The first step here is to take our dataset and remove the name column, then format all the other variables as numeric, a key condition for PCA.

```{r echo=FALSE}


## first change the character variables to numeric for rankings variables ie columns 2:7
## a function to do this selecting the columns of interest except the first which is the names chr variable


## change first column to rownames
ranking_variables_distinct <- data.frame(ranking_variables_distinct, row.names = 1)


i <- c(1:13)                                  # Specify columns you want to change
## We can now use the apply function to change columns 2 to 10 to numeric:
  
ranking_variables_distinct[ , i] <- apply(ranking_variables_distinct[ , i], 2,  # Specify own function within apply
                      function(x) as.numeric(as.character(x)))

str(ranking_variables_distinct)

## remove column name from first column as rownames

ranking_variables_distinct <- ranking_variables_distinct[-1]
row.names(ranking_variables_distinct) <- ranking_variables_distinct$name

str(ranking_variables_distinct)
head(ranking_variables_distinct)

```

### DATA STANDARDISATION
In principal component analysis, variables are often scaled (i.e. standardized). This is a good idea when variables are measured in different scales (e.g: kilograms, kilometers, centimeters, …); otherwise, the PCA outputs obtained will be severely affected.

The goal here is to make the variables comparable. Generally variables are scaled to have: 
i) standard deviation one and 
ii) mean zero
The function PCA() [FactoMineR package] can be used.

This standardization to the same scale avoids some variables to become dominant just because of their large measurement units. It makes variable comparable.

The R code below, computes principal component analysis on the active individuals/variables: 


```{r}
library("FactoMineR")
res.pca1 <- PCA(ranking_variables_distinct, graph = FALSE)
print(res.pca1)
```


### VISUALIZATION AND INTERPRETATION

We will use the factoextra R package to help in the interpretation of PCA. 

Eigenvalues / Variances
The eigenvalues measure the amount of variation retained by each principal component in the dataset.
Eigenvalues are large for the first PCs and small for the subsequent PCs. That is, the first PCs corresponds to the directions with the maximum amount of variation in the data set.

We examine the eigenvalues to determine the number of principal components to be considered. 
The eigenvalues and the proportion of variances (i.e., information) retained by the principal components (PCs) can be extracted using the function get_eigenvalue() from the factoextra package.

```{r echo=FALSE}

library("factoextra")
eig.val <- get_eigenvalue(res.pca1)
eig.val



```

Eigenvalues can be used to determine the number of principal components to retain after PCA (Kaiser 1961):

An eigenvalue > 1 indicates that PCs account for more variance than accounted by one of the original variables in standardized data. 
This is commonly used as a cutoff point for which PCs are retained. This is true only when the data is standardized.

You can also limit the number of components to a number that accounts for a certain fraction of the total variance. So for example here, as can be seen from the eigen values above, we might choose to stop at dimension 7, as up to this point, 95% of the variation is explained.


### SCREE PLOT
An alternative method to determine the number of principal components is to look at a Scree Plot, 
which is the plot of eigenvalues ordered from largest to the smallest. 
The number of component is determined at the point, beyond which the remaining eigenvalues are all relatively small and of comparable size (Jollife 2002, Peres-Neto, Jackson, and Somers (2005)).

The scree plot can be produced using the function fviz_eig() or fviz_screeplot() from the factoextra package.

```{r echo=FALSE}


fviz_eig(res.pca1, addlabels = TRUE, ylim = c(0, 70))

```
From the plot above, we might want to stop at the 7th principal component. 
95% of the information (variances) contained in the data are retained by the first five principal components.PUT RIGHT VALUES IN

### RESULTS
To extract the results for variables from a PCA output we can use the function 
get_pca_var() (factoextra package). This function provides a list of matrices containing all the 
results for the active variables (coordinates, correlation between variables and axes, squared cosine and 
contributions)

The components of the get_pca_var() can be used in the plot of our variables as follows:

var$coord: coordinates of variables to create a scatter plot
var$cos2: represents the quality of representation for variables on the factor map. It’s calculated as the squared coordinates: var.cos2 = var.coord * var.coord.
var$contrib: contains the contributions (in percentage) of the variables to the principal components. The contribution of a variable (var) to a given principal component is (in percentage) : (var.cos2 * 100) / (total cos2 of the component).

```{r echo=FALSE}
## Graph of variables


var <- get_pca_var(res.pca1)
var

## The different components can be accessed as follow:

# Coordinates
head(var$coord)
# Cos2: quality on the factore map
head(var$cos2)
# Contributions to the principal components
head(var$contrib)
```


### CORRELATION CIRCLE
The correlation between a variable and a principal component (PC) is used as the coordinates of the variable on the PC. The representation of variables differs from the plot of the observations: 
The observations are represented by their projections, but the variables are represented by their correlations (Abdi and Williams 2010).
Color by cos2 values: quality on the factor map

```{r echo=FALSE}
# 
fviz_pca_var(res.pca1, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
)
```


The plot above is also known as variable correlation plots. It shows the relationships between all variables.
It can be interpreted as follows:

Positively correlated variables are grouped together. We can see that atp_wins and gs_wins are correlated, as well as atp_losses and gs_losses. This makes sense, as players who do well in most atp tournaments will tend to also perform in the grand slams, although there are some outliers as mentioned previously.
Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants).
Interestingly, ranking points and tournament performance are not correlated, from this data. This is somewhat counterintuitive, as one would suspect that good tournament performance would result in better ranks. However, there would be some players who do well in tournaments which don't offer a high number of ranking points, which would explain why tournament performance and ranking poinjts are not really correlated here.

The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map. Due to the cluster of variables on this correlation plot, it is hard to see the quality for each variable. The cos2 on the key of this plot is explained below.


### QUALITY OF REPRESENTATION

The quality of representation of the variables on factor map is called cos2 (square cosine, squared coordinates).

A high cos2 indicates a good representation of the variable on the principal component. 
In this case the variable is positioned close to the circumference of the correlation circle.

A low cos2 indicates that the variable is not perfectly represented by the PCs. 
In this case the variable is close to the center of the circle.

```{r echo=FALSE}
##  You can access to the cos2 as follow:

head(var$cos2, 4)

## You can visualize the cos2 of variables on all the dimensions using the corrplot package:

library("corrplot")
corrplot(var$cos2, is.corr = FALSE)
```

A bar chart of the cos2 is probably a more user-friendly result:
```{r echo=FALSE}
## It’s also possible to create a bar plot of variables cos2 using the function fviz_cos2()[in factoextra]:

# Total cos2 of variables on Dim.1 and Dim.2
fviz_cos2(res.pca1, choice = "var", axes = 1:2)

```


Contributions of variables to PCs
The contributions of variables in accounting for the variability in a given principal component are expressed in percentage.

Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that are not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis.
The larger the value of the contribution, the more the variable contributes to the component.
```{r echo=FALSE}
## 
##  The contribution of variables can be extracted as follow :

head(var$contrib, 4)

## .
```


```{r echo=FALSE}

library("corrplot")
corrplot(var$contrib, is.corr=FALSE)
```

Once again bar charts is also helpful to visualise which variables are contributing most to the variations.
Here are the contributions of each variable to PC1. 
```{r echo=FALSE}
## 
# Contributions of variables to PC1
fviz_contrib(res.pca1, choice = "var", axes = 1, top = 10)
```

Likewise the contributions to PC2
```{r echo=FALSE}
# Contributions of variables to PC2

fviz_contrib(res.pca1, choice = "var", axes = 2, top = 10)
```


Now the total contribution to PC1 and PC2
```{r echo=FALSE}
## The total contribution to PC1 and PC2 is obtained with the following R code:

fviz_contrib(res.pca1, choice = "var", axes = 1:2, top = 10)
```

Clearly these plots confirm the information plotted in the correlation circle above. The ranking points are correlated with Dim 1 and tournament performance to Dim2.

The red dashed line on the graphs above indicate the expected average contribution. 

### LINEAR REGRESSION TO PREDICT RANKING AT TEN YEAR MARK

We will now perform multiple linear regression to determine the contributions of different variables to rnkng_pnts_10yr, the outcome variable here.

The first model takes into account all possible variables, with ranking points at ten years as the outcome.

```{r echo=FALSE}
##  
##  build the linear regression model

model1 <- lm(rnkng_pnts_10yr ~ rnkng_pnts_6mnths + rnkng_pnts_9mnths + rnkng_pnts_1yr + rnkng_pnts_2yr + rnkng_pnts_3yr +
            rnkng_pnts_4yr + rnkng_pnts_5yr + gs_losses + gs_wins + atp_wins + atp_losses, data = ranking_variables_distinct)
summary(model1)
```

Clearly ranking points at 6 months, and tournament results are quite insignificant here, indicating that as one might expect, ranking can only be loosely predicted by previous rankings at around 5 or 6 years prior to the current date. The R squared value of this regression is a weak 0.5376, telling us that predicting will be unreliable at the best of times. 

We will now remove the variables which don't contribute to the prediction, and build a new model.

```{r echo=FALSE}
model2 <- lm(rnkng_pnts_10yr ~  rnkng_pnts_1yr + rnkng_pnts_2yr + rnkng_pnts_3yr +
               rnkng_pnts_4yr + rnkng_pnts_5yr, data = ranking_variables_distinct)
summary(model2) 
```

Interestingly the model hardly improves, meaning that even though rankikng points in the first 5 years a player is on tour can be somewhat helpful in predicting where they will be in ten years, it is still anybody's guess. 

It can however be seen that the later ranking points (those at 3, 4 and 5 years) are more significant than earlier ones, so a final regression model will be constructed, taking only these into account.

```{r echo=FALSE}

model3 <- lm(rnkng_pnts_10yr ~   rnkng_pnts_3yr + 
               rnkng_pnts_4yr + rnkng_pnts_5yr, data = ranking_variables_distinct)
summary(model3) 
```
Once again, the R squared value does not change, remaining 0.5093.

### CONCLUSIONS

You may be wondering why ranking points at 10 years was chosen as an outcome variable, as it is highly variable. Surely career high ranking or cumulative ranking points would have been more informative outcome variables. The simple answer to this is that there was insufficent data (not to mention time:) ) to obtain these sort of variables.

Overall however, it is plain that trying to predict the chosen outcome variable was unreliable at best. Only a few variables which were closer to the outcome can predict with anything approaching 50% reliability.

It is perhaps not surprising that trying to predict sport in general is highly difficult, as if it was able to be predicted easily, there would be no interest in it. This simply highlights the exciting nature of hthe game of tennis, in that matches can change drastically across a couple of sets of even games or points, and there are so many variables which contribute the outcome of any one match (location, court surface or player injury record or status, for example) that it is very hard to forecast results.

Throughout this analysis it became increasingly evident that one cannot simply trust data given, and the importance of conducting sanity checks throughout the analysis cannot be overestimated, both for detecting errors in ones own analysis and for noticing unrealistic readings in the supplied data. For example, some of the datasets in the deuce package contained multiple missing values making analysis unreliable, and sometimes contained multiple instances of the same player (up to 80 instances of identical players). 
