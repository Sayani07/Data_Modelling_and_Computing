---
title: "ETC 1010 Lab 5 SOLUTION"
output: html_document
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  echo=FALSE,
  comment = "",
  fig.height = 8,
  fig.width = 12,
  fig.align = "center",
  cache = FALSE
)
```

```{r echo=FALSE}
library(tidyverse)
library(forcats)
```

# Instructions

In this week's lab, the main goal is to practice reading and handling different data formats. On the due date, turn in your Rmd file and the html product. 

## Exercise 1

Open your project for this class. Make sure all your work is done relative to this project.

Open the `lab5.Rmd` file provided with the instructions. You can edit this file and add your answers to questions in this document.


## Exercise 2 (9pts)

The financial world operates with many different currencies, and for countries to trade, for people to travel, these currencies are converted, bought and sold. The web site [https://openexchangerates.org/](https://openexchangerates.org/) provides access to (daily) cross rates of currencies relative to the USD. The data is provided in JSON form. With a free account you can pull 30 days worth of data, every day. [Carson Sievert's web site](https://gist.github.com/cpsievert/e05da83fc4253e6d1986) explains how to do this, and provides R code. To follow the instructions and get your own data you need to sign up for an instant free account, and use the key provided to get the data. 

a. (2pts) Between your group members pull at least 80 days of cross-rates. Turn this in with your lab report.
b. (2pts) Use the `lubridate` package data handling routines to make R recognise the date column as a date. `If you write out the data and read it in using read_csv then the date will be recognised as a date.`

```{r}
library(lubridate)
rates <- read_csv("rates.csv")
```

c. Make a time series plot of the currency of your choice, for the period of data that you have downloaded. Describe the fluctuation in the rates over the time. `Something like this: The Indian rupee has declined a small amount of this time, which corresponds to strengthening relative to the USD.`

```{r}
ggplot(rates, aes(x=date, y=INR)) + geom_line()
```

d. (3pts) Compare and contrast the Australian dollar fluctuations with this currency. Make a plot or two to help with your description. `The Indian rupee and the Australian dollar have a very similar pattern, except for the very last point where the rupee gained substantially more relative to the USD.`

```{r}
ggplot(rates, aes(x=AUD, y=INR)) + geom_point() + theme(aspect.ratio=1)
rates %>% select(date, INR, AUD) %>%
  mutate(AUD=scale(AUD), INR=scale(INR)) %>%
  gather(currency, rate, -date) %>%
  ggplot(aes(x=date, y=rate, colour=currency)) + 
  geom_line() + scale_colour_brewer(palette="Dark2")
```

## Exercise 3 (10pts)

This task is to take the example of the Australian electorate data from the lecture notes (Lecture 5 Reading different data formats), make sure that you can do the work, to make an electoral map of Australia. And then extend it with a little more info.

a. Download the Australian electorate shape files.

```{r}
library(eechidna)
library(rgdal)
sF <- readOGR(dsn="../lectures/data/national-midmif-09052016/COM_ELB.TAB", layer="COM_ELB")
```

b. Use the `mapshaper` package to thin out the boundaries.

```{r echo=TRUE}
library(rmapshaper)
sFsmall <- ms_simplify(sF, keep=0.05)
```

c. Get the map polygons into tidy format.

```{r echo=TRUE}
nat_map <- ggplot2::fortify(sFsmall)
```

d. (3pts) Make the map of electorates coloured by size.


```{r echo=TRUE}
library(ggthemes)
nat_data <- sF@data
nat_data$id <- row.names(nat_data)
nat_map$group <- paste("g",nat_map$group,sep=".")
nat_map$piece <- paste("p",nat_map$piece,sep=".")
nms <- sFsmall@data %>% select(Elect_div, State)
nms$id <- as.character(1:150)
nat_map <- left_join(nat_map, nms, by="id")
ggplot(aes(map_id=id), data=nat_data) +
  geom_map(aes(fill=Area_SqKm), map=nat_map) +
  expand_limits(x=nat_map$long, y=nat_map$lat) + 
  theme_map()
```

e. Extract the information about the electorates from the shape files. Extract the centroids for each electorate, and combine these two.


```{r echo=TRUE}
polys <- as(sF, "SpatialPolygons")
centroid <- function(i, polys) {
  ctr <- Polygon(polys[i])@labpt
  data.frame(long_c=ctr[1], lat_c=ctr[2])
}
centroids <- seq_along(polys) %>% purrr::map_df(centroid, polys=polys)
nat_data <- bind_cols(nat_data, centroids)
```

f. (3pts) Plot the centroids overlaid on the map of electorates.

```{r}
ggplot(aes(map_id=id), data=nat_data) +
  geom_map(aes(fill=Area_SqKm), map=nat_map) +
  expand_limits(x=nat_map$long, y=nat_map$lat) + 
  theme_map() + theme(legend.position="none") +
  geom_point(data=nat_data, aes(x=long_c, y=lat_c), colour="orange")
```

g. (4pts) Pull the polling station data from the AEC site. Plot the polling stations on the map. Explain any patterns that you see in locations where you can go to vote.

`This site has the polling places:` [http://www.aec.gov.au/About_AEC/cea-notices/election-pp.htm](http://www.aec.gov.au/About_AEC/cea-notices/election-pp.htm)

`The polling places are concentrated heavily on the coastlines, as expected. There are some stripes in the rural areas which look like they are located along major roads. There are very few places in the extremely rural areas - people must have to postal vote!`

```{r}
poll_places <- read_csv("prdelms.gaz.statics.170801.09.00.01.csv")
poll_places <- poll_places %>% 
  filter(Long>0, Lat<0)
ggplot(aes(map_id=id), data=nat_data) +
  geom_map(aes(fill=Area_SqKm), map=nat_map) +
  expand_limits(x=nat_map$long, y=nat_map$lat) + 
  theme_map() + theme(legend.position="none") +
  geom_point(data=poll_places, aes(x=Long, y=Lat), 
             colour="orange", alpha=0.5, inherit.aes = FALSE)
```

## Exercise 4 (11pts)

In previous labs, we worked with the PISA 2015 data. In this question, you will extract this data for yourself. 

a. Find the data on the OECD PISA web site, [http://www.oecd.org/pisa/data/2015database/](http://www.oecd.org/pisa/data/2015database/). Download the SPSS format "Student questionnaire data file (419MB)". (The downloaded file name should be `CY6_MS_CMB_STU_QQQ.sav`.) It is quite large, so if you have trouble your tutor has the file on a USB stick, that you can copy.

b. (1pt) Read the data into R using this code:

```{r eval=FALSE}
library(haven)
pisa_2015 <- read_sav(file.choose())
```

How many students are in the data set? `519334`

c. If you continue to work with the data in R you will have some slow times. It is ok if you just want to focus on one country, but if we want to make calculations and models on all the data you computer will sit and spin a lot. The best approach is to create a small database, and use this to do calculations. This code creates the database, in your project folder:

```{r eval=FALSE}
library(sqldf)
library(DBI)
dbWriteTable(conn=db, name="student", value=pisa_2015)
```

d. (2pts) Let's test the speed

From the R object:

```{r eval=FALSE}
start_time <- Sys.time()
score_gap <- pisa_2015 %>% 
  group_by(CNT) %>%
  summarise(math=mean(PV1MATH),
            read=mean(PV1READ),
            scie=mean(PV1SCIE))
end_time <-Sys.time()
end_time-start_time
```

Using sqlite database:

```{r}
library(sqldf)
library(DBI)
db <- dbConnect(SQLite(), dbname="PISA.sqlite")
tb <- tbl(db, "student")
```

```{r eval=FALSE}
start_time <- Sys.time()
score_gap <- tb %>% group_by(CNT) %>%
  summarise(math=mean(PV1MATH),
            read=mean(PV1READ),
            scie=mean(PV1SCIE)) %>%
  collect()
end_time <-Sys.time()
end_time-start_time
```

`I get 27.09944 secs directly in R, and 1.61168 secs using the database. It could vary between computers.`

e. (2pts) Using the code below, how many different plausible scores are generated for each student, in math, reading and science? `10`

```{r eval=FALSE}
dbListFields(db, "student")
```

f. (3pts) Compute the averages across the multiple math scores, and save in an R object. Make a dotplot against country, ordered from top score to lowest. What are the top three countries? What is Australia's rank?

`Singapore, Hong Kong, Macao - report these as names not three letter codes.`

`The code 
scores %>% mutate(order=order(mathmean, decreasing=TRUE)) %>% filter(CNT == "AUS") 
gives Australia as 41 out of 73.`

```{r}
scores <- tb %>% 
  mutate(math=(PV1MATH+PV2MATH+PV3MATH+PV4MATH+PV5MATH+PV6MATH+
                 PV7MATH+PV8MATH+PV9MATH+PV10MATH)/10) %>%
  group_by(CNT) %>%
  summarise(mathmean=mean(math)) %>%
  collect()
scores <- scores %>% mutate(CNT=fct_reorder(CNT, mathmean))
ggplot(scores, aes(x=mathmean, y=CNT)) + geom_point()
```

g. (3pts) Database operations typically only operate on a column by column basis, so calculating statistics such as standard deviation can be a challenge. (Try it, and see what happens if you ask for the database to compute the standard deviation of the math scores instead of the mean, using the `sd` function.) You can do this with a direct SQL QUERY (the ugly code is below). Do it! And then make a plot which shows the mean and a segment indicating one standard deviation below and above the mean, by country, sorted from highest to lowest average.


```{r}
library(RSQLite)
mathmean <- dbGetQuery(db, "SELECT Avg((PV1MATH+PV2MATH+PV3MATH+PV4MATH+PV5MATH+PV6MATH+
                 PV7MATH+PV8MATH+PV9MATH+PV10MATH)/10) as m FROM student GROUP BY CNT")
mathsq <- dbGetQuery(db, "SELECT Sum((PV1MATH+PV2MATH+PV3MATH+PV4MATH+PV5MATH+PV6MATH+
        PV7MATH+PV8MATH+PV9MATH+PV10MATH)*(PV1MATH+PV2MATH+PV3MATH+PV4MATH+PV5MATH+PV6MATH+
        PV7MATH+PV8MATH+PV9MATH+PV10MATH)/100) as s FROM student GROUP BY CNT")
n <- dbGetQuery(db, "SELECT count(*) as n FROM student GROUP BY CNT")
CNT <- dbGetQuery(db, "SELECT distinct CNT as CNT FROM student")

scores_sql <- data.frame(mathmean, mathsq, n, CNT)
scores_sql <- scores_sql %>%
  mutate(mathsd=sqrt((s - n*m^2)/(n-1))) %>%
  mutate(CNT=fct_reorder(CNT, m))
ggplot(scores_sql, aes(x=CNT, y=m)) + 
  geom_point() +
  geom_linerange(aes(ymin=m-mathsd, ymax=m+mathsd)) +
  coord_flip()
```
