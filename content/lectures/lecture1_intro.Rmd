---
title: "ETC1010: Data Modelling and Computing"
subtitle: 'Lecture 1: Introduction'
author: "Di Cook (dicook@monash.edu, @visnut)"
date: "7/25/2017"
output:
  xaringan::moon_reader:
    css: ["default", "myremark.css"]
    self_contained: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r, echo = FALSE, warning = FALSE, message=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  echo=FALSE,
  comment = "",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
library(tidyverse)
library(gridExtra)
library(plotly)
library(ggthemes)
```

# Welcome to Semester 2 2017!

- __Lecturers__: [Professor Di Cook](http://dicook.org) and [Dr David Frazier](http://monash.edu/research/explore/en/persons/david-frazier(b3a84d85-75f7-4ce8-8732-f7efcc5d772c.html)
- __Tutors__: Puwasala Gamakumara, Mitch O'Hara-Wild, Yeasmin Mahbuba
--

- __Unit guide__: Objectives, tentative schedule, grading
--

- __Textbook__: [R for Data Science](http://r4ds.had.co.nz), Garret Grolemund and Hadley Wickham
--

- __Computing__: R and RStudio
--

- __Materials__: Moodle for grades, online quizzes; [course web site](https://dmac.netlify.com) for lecture and lab materials

---
# Outline

- What is data? 
- What can we do if we have good data handling skills?
    - __Insert lots of examples here__
- Why R?

---
# Music

- Read in a wave file
- Digitise it as time and amplitude
- Plot
- Compare with other sounds

```{r}
library(tuneR)
m1 <- readWave("data/data3.wav")
m1 <- extractWave(m1, from = 25000, to = 75000)
m1_df <- data.frame(t=1:length(m1@left), 
                    left=m1@left, 
                    right=m1@right)
p1 <- ggplot(m1_df, aes(x=t, y=left)) + geom_line() 
m2 <- readWave("data/statistics1.wav")
m2 <- extractWave(m2, from = 25000, to = 75000)
m2_df <- data.frame(t=1:length(m2@left), 
                    left=m2@left, 
                    right=m2@right)
p2 <- ggplot(m2_df, aes(x=t, y=left)) + geom_line()
grid.arrange(p1, p2, ncol=1)
```
---
# Compare left and right channels

```{r}
p1 <- ggplot(m1_df, aes(x=left, y=right)) + 
  geom_point() + theme(aspect.ratio=1)
p2 <- ggplot(m2_df, aes(x=left, y=right)) + 
  geom_point() + theme(aspect.ratio=1)
grid.arrange(p1, p2, ncol=2)
```

Oh, same sound is on both channels! A tad drab.

---
# Compute statistics

```{r}
m1_df <- m1_df %>% gather(channel, value, left, right) %>%
  mutate(word="data")
m2_df <- m2_df %>% gather(channel, value, left, right) %>%
  mutate(word="statistics")
m_df <- bind_rows(m1_df, m2_df)
m_df %>% filter(channel == "left") %>%
  group_by(word) %>%
  summarise(m = mean(value), s = sd(value), 
            mx = max(value), mn = min(value))
```

---
# My music - don't laugh

```{r}
music <- read.csv("data/music-sub.csv", 
                  row.names=1, stringsAsFactors = FALSE)
ggplot(music, aes(x=artist, y=lave)) + geom_boxplot() +
  xlab("") + ylab("Average amplitude")
```

---

```{r fig.height=6, fig.weight=6}
ggplot(music, aes(x=lvar, y=lave, colour=artist)) + 
  geom_point(size=5, alpha=0.5) +
  scale_colour_brewer(palette="Dark2") +
  xlab("Std amplitude") + ylab("Average amplitude") +
  theme(aspect.ratio = 1)
```

Abba is just different from everyone else!

---
# Education

- OECD PISA survey is the world's global metric for quality, equity and efficiency in school education.
- Workforce readiness of 15-year old students
- 14530 students tested in Australia in 2015
- How many schools?
- Math, reading and science tests, surveys on school and home environment, 921 variables
- Data available from [http://www.oecd.org/pisa/data/](http://www.oecd.org/pisa/data/)

```{r eval=FALSE}
pisa_2015 <- read_sav(file.choose()) # The SPSS format zip file
pisa_au <- pisa_2015 %>% filter(CNT == "AUS")
save(pisa_au, file="data/pisa_au.rda")
```


---
# Gender differences

```{r}
load("data/pisa_au.rda")
pisa_au <- pisa_au %>% 
  mutate(ST004D01T = factor(ST004D01T, 
                            levels=c(1,2), 
                            labels=c("girls", "boys"))) 
pisa_au %>%
  group_by(ST004D01T) %>%
  summarise(math=weighted.mean(PV1MATH, SENWT), 
            read=weighted.mean(PV1READ, SENWT), 
            sci=weighted.mean(PV1SCIE, SENWT))
p1 <- ggplot(pisa_au, aes(x=ST004D01T, y=PV1MATH, 
                          fill=ST004D01T)) +
  geom_boxplot() + xlab("") + theme(legend.position="none")
p2 <- ggplot(pisa_au, aes(x=ST004D01T, y=PV1READ, 
                          fill=ST004D01T)) +
  geom_boxplot() + xlab("") + theme(legend.position="none")
p3 <- ggplot(pisa_au, aes(x=ST004D01T, y=PV1SCIE, 
                          fill=ST004D01T)) +
  geom_boxplot() + xlab("") + theme(legend.position="none")
grid.arrange(p1, p2, p3, ncol=3)
```

On average reading scores are most different, with girls scoring substantially higher. There is more variability from individual to individual than from boys to girls. 

---
class: inverse middle 
# Your turn

Point out a couple more things that you see in terms of differences between girls and boys?

---
# Tennis

Statistics for grand slam and top tournament matches are available through the ATP and WTA web sites. By web scraping these web sites we can pull data together to explore characteristics of different players, surfaces, ... These are compiled into the R package deuce. 

```{r}
# devtools::install_github("skoval/deuce")
library(deuce)
data(wta_matches)
grand_slams <- filter(wta_matches, 
                      tourney_level == "Grand Slams", 
                      year>1983)
p <- ggplot(grand_slams, aes(x=factor(year), y=winner_age)) + 
  geom_boxplot() + 
  scale_x_discrete(breaks=c("1990", "2000", "2010", "2020")) + 
  xlab("") + ylab("winner age") 
p <- p +
  geom_point(mapping=aes(text=winner_name), 
             data=filter(grand_slams, winner_age > 35), 
             alpha=0.5, colour="red", size=3) 
ggplotly(p)
```

Yes, there are 47 year olds playing Grand Slams! [See NYTimes, 2004](http://www.nytimes.com/2004/06/21/sports/tennis/navratilova-47-wins-at-wimbledon.html)


---
# Politics

The [Australian Electoral Commission](http://www.aec.gov.au/elections/federal_elections/2016/downloads.htm) provides Federal election results, and the electoral map. 

```{r fig.width=5, fig.height=4}
library(eechidna)
who_won <- aec2016_2pp_electorate %>% 
  group_by(PartyNm) %>% 
  tally() %>% 
  arrange(desc(n)) 

# plot
library(ggplot2)
library(scales)
who_won <- aec2016_2pp_electorate %>% 
  group_by(PartyNm) %>% 
  tally() %>% 
  arrange(desc(n)) 

# plot
ggplot(who_won, 
       aes(reorder(PartyNm, n), 
           n)) +
  geom_point(size = 3) + 
  coord_flip() + 
  scale_y_continuous(labels = comma) +
  theme_bw() +
  ylab("Total number of electorates") +
  xlab("") +
  theme(text = element_text(size=10))
```

---
# Map it!

```{r fig.width=7, fig.height=5}
data(nat_data_2016_cart)
data(nat_map_2016)
data(aec2016_fp_electorate)
map.winners <- aec2016_fp_electorate %>% filter(Elected == "Y") %>% 
  select(Electorate, PartyNm) %>% 
  merge(nat_map_2016, by.x="Electorate", by.y="Elect_div")

# Grouping different Lib/Nats togethers
map.winners$PartyNm <- as.character(map.winners$PartyNm)
coalition <- c("Country Liberals (NT)", "Liberal", 
               "Liberal National Party of Queensland", "The Nationals")
map.winners.grouped <- mutate(map.winners, 
    PartyNm = ifelse(as.character(PartyNm) %in% coalition,
       "Liberal National Coalition", PartyNm))
map.winners.grouped <- map.winners.grouped %>% arrange(group, order)

# Colour cells to match that parties colours
# Order = Australian Labor Party, Independent, Katters, Lib/Nats Coalition, Palmer, The Greens
partycolours = c("#FF0033", "#000000", "#CC3300", "#0066CC", "#FFFF00", "#009900")

ggplot(data=map.winners.grouped) + 
  geom_polygon(aes(x=long, y=lat, group=group, order=order, fill=PartyNm)) +
  scale_fill_manual(name="Politcal Party", values=partycolours) +
  theme_map() + coord_equal() + theme(legend.position="bottom")
```

---
# Cartogram it!

```{r fig.width=7, fig.height=5}
# Load election results
cart.winners <- aec2016_fp_electorate %>% filter(Elected == "Y") %>% 
  select(Electorate, PartyNm) %>% 
  merge(nat_data_2016_cart, by.x="Electorate", by.y="ELECT_DIV")

# Grouping different Lib/Nats togethers
cart.winners$PartyNm <- as.character(cart.winners$PartyNm)
coalition <- c("Country Liberals (NT)", "Liberal", "Liberal National Party of Queensland",
               "The Nationals")
cart.winners.grouped <- mutate(cart.winners, 
  PartyNm = ifelse(as.character(PartyNm) %in% coalition, 
                   "Liberal National Coalition", PartyNm))

# Plot it
ggplot(data=nat_map_2016) +
  geom_polygon(aes(x=long, y=lat, group=group, order=order),
               fill="grey90", colour="white") +
  geom_point(data=cart.winners.grouped, aes(x=x, y=y, colour=PartyNm), size=2, alpha=0.8) +
  scale_colour_manual(name="Political Party", values=partycolours) +
  theme_map() + coord_equal() + theme(legend.position="bottom")
```

---
# Combine with Census data

```{r}
library(vembedr)
embed_vimeo("167367369")
```

---
# Pedestrian sensors

---
# Google maps

---
# Airline traffic

---
# Exchange rates

---
# Text

Is it possible to distinguish tweets coming from Donald Trump's phone vs his staff's phone? With a twitter api you can collect all tweets between certain times, from different people, with different hashtags, ... David Robinson wrote a [post](http://varianceexplained.org/r/trump-tweets/) during last year's US election cycle doing just this. Here's a re-creation of his analysis.

Tweets from @realDonaldTrump were collected and passed through a sentiment analysis.

```{r}
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
tweets <- trump_tweets_df %>%
  select(id, statusSource, text, created) %>%
  extract(statusSource, "source", "Twitter for (.*?)<") %>%
  filter(source %in% c("iPhone", "Android"))
tweets %>%
  count(source, hour = hour(with_tz(created, "EST"))) %>%
  mutate(percent = n / sum(n)*100) %>%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "")
```

---
# Common words between devices

```{r fig.height=7}
library(tidytext)
library(stringr)

reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
android_iphone_ratios <- tweet_words %>%
  count(word, source) %>%
  filter(sum(n) >= 5) %>%
  spread(source, n, fill = 0) %>%
  mutate_at(vars(Android, iPhone), 
            funs((. + 1) / sum(. + 1))) %>%
  mutate(logratio = log2(Android / iPhone)) %>%
  arrange(desc(logratio))
android_iphone_ratios %>%
  group_by(logratio > 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Android / iPhone log ratio") +
  scale_fill_manual(name = "", labels = c("Android", "iPhone"),
                    values = c("red", "lightblue"))
```

---
# Sentiment analysis

Poisson test of the differences between whether it is mode likely to emerge from the Android.

```{r results='hide'}
nrc <- sentiments %>%
  filter(lexicon == "nrc") %>%
  dplyr::select(word, sentiment)

sources <- tweet_words %>%
  group_by(source) %>%
  mutate(total_words = n()) %>%
  ungroup() %>%
  distinct(id, source, total_words)

by_source_sentiment <- tweet_words %>%
  inner_join(nrc, by = "word") %>%
  count(sentiment, id) %>%
  ungroup() %>%
  complete(sentiment, id, fill = list(n = 0)) %>%
  inner_join(sources) %>%
  group_by(source, sentiment, total_words) %>%
  summarize(words = sum(n)) %>%
  ungroup()

library(broom)
sentiment_differences <- by_source_sentiment %>%
  group_by(sentiment) %>%
  do(tidy(poisson.test(.$words, .$total_words))) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, estimate))
```

```{r fig.width=6, fig.height=5}
ggplot(sentiment_differences, aes(x=sentiment, y=estimate)) + 
  geom_hline(yintercept=1, colour="white", size=3) +
  geom_point() + 
  geom_errorbar(aes(x=sentiment, ymin=conf.low, ymax=conf.high)) +
  scale_y_continuous(breaks=seq(0.7, 2.2, 0.1)) + 
  coord_flip()
```

---
# Climate change

- Data is collected at a number of locations world wide. 
- See [Scripps Inst. of Oceanography](http://scrippsco2.ucsd.edu/data/atmospheric_co2) 
- Let's pull the data from the web and take a look ...
- 
- Recordings from South Pole (SPO), Kermadec Islands (KER), Mauna Loa Hawaii (MLF), La Jolla Pier, California (LJO), Point Barrow, Alaska (PTB).

---

```{r CO2, fig.width=10, fig.height=5}
CO2.ptb<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_ptb.csv", sep=",", skip=69)
colnames(CO2.ptb)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.ptb$lat<-71.3
CO2.ptb$lon<-(-156.6)
CO2.ptb$stn<-"ptb"

CO2.ljo<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_ljo.csv", sep=",", skip=69)
colnames(CO2.ljo)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.ljo$lat<-32.9
CO2.ljo$lon<-(-117.3)
CO2.ljo$stn<-"ljo"

CO2.spo<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_spo.csv", sep=",", skip=69)
colnames(CO2.spo)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.spo$lat<- (-90.0)
CO2.spo$lon<-0
CO2.spo$stn<-"spo"

CO2.ker<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_ker.csv", sep=",", skip=69)
colnames(CO2.ker)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.ker$lat<-(-29.2)
CO2.ker$lon<-(-177.9)
CO2.ker$stn<-"ker"

CO2.all<-rbind(CO2.ker,CO2.ljo,CO2.ptb,CO2.spo)
CO2.all$date<-as.Date(CO2.all$date)

CO2.all$invlat=-1*CO2.all$lat
CO2.all$stn=reorder(CO2.all$stn,CO2.all$invlat)

CO2.all.loc <- rbind(CO2.ker[1,],CO2.ljo[1,],CO2.ptb[1,],CO2.spo[1,])

p1 <- qplot(date, co2, data=subset(CO2.all, flg < 2), colour=stn, geom="line",xlab="Year",ylab="CO2 (ppm)") + 
		facet_wrap(~stn, ncol=1) + theme(axis.text.y=element_text(size = 6), legend.position="none")
p2 <- qplot(date, co2, data=subset(CO2.all, flg < 2), colour=stn, geom="line",xlab="Year",ylab="CO2 (ppm)") + 
  theme(axis.text.y=element_text(size = 6), legend.position="none")
grid.arrange(p1, p2, ncol=2)
```

---

```{r CO2-map, fig.width=4.5, fig.height=2.5}
world <- map_data("world")
worldmap <- ggplot(world, aes(x=long, y=lat, group=group)) +
  geom_path(color="grey80", size=0.5) + xlab("") + ylab("") +
  scale_y_continuous(breaks=(-2:2) * 30) +
  scale_x_continuous(breaks=(-4:4) * 45) +
  theme_bw() + theme(aspect.ratio=0.6)
worldmap + geom_point(data=CO2.all.loc, aes(x=lon, y=lat, group=1), colour="red", 
                      size=2, alpha=0) +
  geom_text(data=CO2.all.loc, aes(x=lon, y=lat, label=stn, group=1), 
            colour="orange", size=5)
```

- CO2 is increasing, and it looks like it is exponential increase. 
- The same trend is seen at every location - REALLY? Need some physics to understand this.
- Some stations show seasonal pattern - actually the more north the more seasonality - WHY?

---
# R is ...

* Most commonly used data science software [kdnuggets](http://www.kdnuggets.com/2017/05/poll-analytics-data-science-machine-learning-software-leaders.html)
* __Free__ to use, __open source__ so you can see what code is doing to your data
* __Extensible__: Over 11000 user contributed add-on packages currently on CRAN! Bioconductor has more than 1300 packages, and many researchers provide packages through github.
* __Powerful__: With the right tools, get more work done, faster, better.
* __Flexible__: Not a question of _can_, but _how_.

```{r, eval = FALSE, echo = FALSE}
# devtools::install_github("metacran/crandb")
# pkgs <- crandb::list_packages(limit = 999999)
# length(pkgs)
# [1] 11075
```

---
# RStudio is ...

[From Julie Lowndes](http://jules32.github.io/resources/RStudio_intro/):  

<blockquote>
<b>If R were an airplane, RStudio would be the airport</b>, providing many, many supporting services that make it easier for you, the pilot, to take off and go to awesome places. Sure, you can fly an airplane without an airport, but having those runways and supporting infrastructure is a game-changer.
</blockquote>

---
# The RStudio IDE

- Source editor: Docking station for multiple files, Useful shortcuts ("Knit"), Highlighting/Tab-completion, Code-checking (R, HTML, JS), Debugging features  
- Console window: Highlighting/Tab-completion, Search recent commands
- Other tabs/panes: Graphics, R documentation, Environment pane, File system navigation/access, Tools for package development, git, etc

---
# Installing packages

From CRAN

```{r eval=FALSE, echo=TRUE}
install.packages("learnr")
```


From bioconductor

```{r eval=FALSE, echo=TRUE}
source("https://bioconductor.org/biocLite.R")
biocLite("ggbio")
```

From github repos

```{r eval=FALSE, echo=TRUE}
devtools::install_github("earowang/sugrrants")
devtools::install_github("haleyjeppson/ggmosaic")
```

---
# What is R Markdown?

From the [R Markdown home page](http://rmarkdown.rstudio.com/):
- R Markdown is an authoring format that enables easy creation of dynamic documents, presentations, and reports from R. 
- It combines the core syntax of __markdown__ (an easy-to-write plain text format) __with embedded R code chunks__ that are run so their output can be included in the final document. 
- R Markdown documents are fully reproducible (they can be automatically regenerated whenever underlying R code or data changes).

---
# Open data, open source

- Data is available everywhere today, publicly, free
- Software, very powerful software, for analysis of data is available publicly, free
- Combined with a knowledge of mathematics and statistics empowers each of us to contribute to understand and improve our world

---
# This course

__Data preparation accounts for about 80% of the work of data scientists__

[Gil Press, Forbes, 2016](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#47cbbbf46f63)

This is one of the least taught parts of data science, and business analytics, and yet it is what data scientists spend most of their time on. By the end of this semester, we hope that you will have the tools to be more efficient and effective in this area, so that you have more time to spend on your mining and modeling.

---
# Follow along during class

- __These slides__ are made in rmarkdown, using styling from the [xaringan](https://github.com/yihui/xaringan) package. 
- If you download the `Rmd` file of lecture notes ahead of, or during class, you can run the code chunks as I talk and check my calculations or models.


---
class: inverse middle 
# Share and share alike

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.

